{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Data Manipulation\n",
    "\n",
    "\n",
    "In order to train neural networks, we first need to store and manipulate datasets.\n",
    "So let us generate synthetic datasets and store them in a $n$-dimensional array, which is also called the *tensor*.\n",
    "\n",
    "NumPy is the most widely-used scientific computing package in Python. it uses `ndarray` to store and manipulate tabular data (or matrices). \n",
    "\n",
    "No matter which framework you use, the *tensor class* (`ndarray` in MXNet,\n",
    "`Tensor` in both PyTorch and TensorFlow) is similar to NumPy's `ndarray` with\n",
    "additional features.\n",
    "\n",
    "First, the *tensor class* in TensorFlow supports GPU to accelerate the computation\n",
    "whereas NumPy only supports CPU computation.\n",
    "Second, the *tensor class* supports automatic differentiation.\n",
    "\n",
    "These properties make the *tensor class* suitable for deep learning. In this course, we refer to tensors as instances of the tensor class unless otherwise stated.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "The following sections will revisit the mathematical concepts of this lesson iwith practical examples in Python and TensorFlow. You will learn how the basic math and numerical computing are easily implemented with library functions that you will build on as you progress through the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "To start, we import `tensorflow`. \n",
    "\n",
    "As the name is a little long, we often import\n",
    "it with a short alias `tf`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 7
   },
   "source": [
    "Definition:\n",
    "> A tensor represents a (possibly multi-dimensional) array of numerical values.\n",
    "\n",
    "- A tensor with  one axis corresponds (in math) to a *vector*.\n",
    "- A tensor with  two axes corresponds to a *matrix*.\n",
    "- Tensors with more than two axes do not have special mathematical names.\n",
    "\n",
    "To start, we can use `arange` to create a row vector `x`\n",
    "containing the first 12 integers starting with 0,\n",
    "though they are created as floats by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.range(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the values in the tensor is called an *element* of the tensor.\n",
    "\n",
    "For instance, there are 12 elements in the tensor `x`.\n",
    "\n",
    "Unless otherwise specified, a new tensor\n",
    "will be stored in main memory and designated for CPU-based computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "Let us access a tensor's *shape* which designates the length along each axis\n",
    "To do so, we call the `shape` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "If we just want to know the total number of elements in a tensor,\n",
    "i.e., the product of all of the shape elements,we can inspect its size.\n",
    "\n",
    "Since our vector x has a single element, its `shape` is identical to its size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "origin_pos": 16,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=12>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.size(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "To **change the shape of a tensor without altering\n",
    "either the number of elements or their values**,\n",
    "we can invoke the `reshape` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "origin_pos": 19,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11]])>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.reshape(x, (3, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "In this example, we  transform our tensor, `x`, from a row vector with shape (12,) to a matrix with shape (3, 4).\n",
    "\n",
    "This new tensor contains the exact same values, but views them as a matrix organized as 3 rows and 4 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "To reiterate, although the shape has changed, the elements have not.\n",
    "Note that the size is unaltered by reshaping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "Reshaping by manually specifying every dimension is unnecessary.\n",
    "If our target shape is a matrix with shape (`height`, `width`),\n",
    "then after we know the width, the height is given implicitly.\n",
    "\n",
    "Why should we have to perform the division ourselves?\n",
    "In the example above, to get a matrix with 3 rows,\n",
    "we specified both that it should have 3 rows and 4 columns.\n",
    "Fortunately, tensors can automatically work out one dimension given the rest.\n",
    "We invoke this capability by placing `-1` for the dimension\n",
    "that we would like tensors to automatically infer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "In our case, instead of calling `x.reshape(3, 4)`,\n",
    "we could have equivalently called `x.reshape(-1, 4)` or `x.reshape(3, -1)`.\n",
    "\n",
    "Typically, we will want our matrices initialized\n",
    "either with zeros, ones, some other constants,\n",
    "or numbers randomly sampled from a specific distribution.\n",
    "*We can create a tensor representing a tensor with all elements\n",
    "set to 0* and a shape of (2, 3, 4) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 23,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "Similarly, we can create tensors with each element set to 1 as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 27,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "Often, we want to *randomly sample the values\n",
    "for each element in a tensor** from some probability distribution.\n",
    "\n",
    "For example, when we construct arrays to serve\n",
    "as parameters in a neural network, we will\n",
    "typically initialize their values randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "origin_pos": 31,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[-0.29266113,  0.616908  , -0.5854539 ,  0.6873052 ],\n",
       "       [-0.61713433, -1.8821822 ,  0.9895492 , -0.7166444 ],\n",
       "       [-0.2591582 ,  0.3229179 , -1.1914335 ,  0.19949503]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal(shape=[3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "This snippet creates a tensor with shape (3, 4).\n",
    "\n",
    "Each of its elements is randomly sampled from a standard Gaussian (normal) distribution with a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 32
   },
   "source": [
    "We can also [**specify the exact values for each element**] in the desired tensor\n",
    "by supplying a Python list (or list of lists) containing the numerical values.\n",
    "\n",
    "Here, the outermost list corresponds to axis 0, and the inner list to axis 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "origin_pos": 35,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
       "array([[2, 1, 4, 3],\n",
       "       [1, 2, 3, 4],\n",
       "       [4, 3, 2, 1]])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 36
   },
   "source": [
    "## Elementwise Operations\n",
    "\n",
    "In deep learning we not only read and write data from/to arrays. we also perform mathematical operations on those arrays.\n",
    "\n",
    "**Elementwise operations:**\n",
    "\n",
    "Some of the simplest and most useful operations\n",
    "are the *elementwise* operations.\n",
    "These apply a standard scalar operation\n",
    "to each element of an array.\n",
    "For functions that take two arrays as inputs,\n",
    "elementwise operations apply some standard binary operator\n",
    "on each pair of corresponding elements from the two arrays.\n",
    "We can create an elementwise function from any function\n",
    "that maps from a scalar to a scalar.\n",
    "\n",
    "- *unary* scalar operator\n",
    "    - In mathematical notation, we denote a *unary* scalar operator (taking one input)by the signature $f: \\mathbb{R} \\rightarrow \\mathbb{R}$. This just means that the function is mapping\n",
    "from any real number ($\\mathbb{R}$) onto another.\n",
    "\n",
    "- *binary* scalar operator:\n",
    "    - We denote a *binary* scalar operator (taking two real inputs, and yielding one output) by the signature $f: \\mathbb{R}, \\mathbb{R} \\rightarrow \\mathbb{R}$.\n",
    "Given any two vectors $\\mathbf{u}$ and $\\mathbf{v}$ *of the same shape*,\n",
    "and a binary operator $f$, we can produce a vector\n",
    "$\\mathbf{c} = F(\\mathbf{u},\\mathbf{v})$\n",
    "by setting $c_i \\gets f(u_i, v_i)$ for all $i$,\n",
    "where $c_i, u_i$, and $v_i$ are the $i^\\mathrm{th}$ elements\n",
    "of vectors $\\mathbf{c}, \\mathbf{u}$, and $\\mathbf{v}$.\n",
    "Here, we produced the vector-valued\n",
    "$F: \\mathbb{R}^d, \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$\n",
    "by *lifting* the scalar function to an elementwise vector operation.\n",
    "\n",
    "**Arithmetic operators**\n",
    "\n",
    "- The common standard arithmetic operators\n",
    "(`+`, `-`, `*`, `/`, and `**`)\n",
    "have all been *lifted* to elementwise operations for any identically-shaped tensors of arbitrary shape.\n",
    "\n",
    "- We can call elementwise operations on any two tensors of the same shape.\n",
    "In the following example, we use commas to formulate a 5-element tuple,\n",
    "where each element is the result of an elementwise operation.\n",
    "\n",
    "***Elementwise Operations***\n",
    "\n",
    "The common standard arithmetic operators\n",
    "(`+`, `-`, `*`, `/`, and `**`)\n",
    "have all been *lifted* to elementwise operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "origin_pos": 39,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 3.,  4.,  6., 10.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.,  0.,  2.,  6.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 2.,  4.,  8., 16.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.5, 1. , 2. , 4. ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.,  4., 16., 64.], dtype=float32)>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([1.0, 2, 4, 8])\n",
    "y = tf.constant([2.0, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x**y  # The ** operator is exponentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "Many (**more operations can be applied elementwise**),\n",
    "including unary operators like exponentiation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "origin_pos": 43,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=\n",
       "array([2.7182817e+00, 7.3890562e+00, 5.4598148e+01, 2.9809580e+03],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 44
   },
   "source": [
    "## Linear Algebra operations\n",
    "In addition to elementwise computations,\n",
    "- we can also perform linear algebra operations, including `vector dot products` and `matrix multiplication`.\n",
    "\n",
    "\n",
    "- We can also concatenate multiple tensors together, stacking them end-to-end to form a larger tensor. We just need to provide a list of tensors\n",
    "and tell the system along which axis to concatenate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 44
   },
   "source": [
    "The example below shows what happens when we concatenate\n",
    "two matrices along rows (axis 0, the first element of the shape)\n",
    "vs. columns (axis 1, the second element of the shape).\n",
    "\n",
    "We can see that the first output tensor's axis-0 length ($6$)\n",
    "is the sum of the two input tensors' axis-0 lengths ($3 + 3$);\n",
    "while the second output tensor's axis-1 length ($8$) is the sum of the two input tensors' axis-1 lengths ($4 + 4$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "origin_pos": 47,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(6, 4), dtype=float32, numpy=\n",
       " array([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [ 2.,  1.,  4.,  3.],\n",
       "        [ 1.,  2.,  3.,  4.],\n",
       "        [ 4.,  3.,  2.,  1.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 8), dtype=float32, numpy=\n",
       " array([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.reshape(tf.range(12, dtype=tf.float32), (3, 4))\n",
    "Y = tf.constant([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "tf.concat([X, Y], axis=0), tf.concat([X, Y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 48
   },
   "source": [
    "Sometimes, we want to construct a binary tensor via *logical statements*.\n",
    "\n",
    "\n",
    "Take `X == Y` as an example.\n",
    "\n",
    "For each position, if `X` and `Y` are equal at that position,\n",
    "the corresponding entry in the new tensor takes a value of 1,\n",
    "meaning that the logical statement `X == Y` is true at that position;\n",
    "otherwise that position takes 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "origin_pos": 49,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=bool, numpy=\n",
       "array([[False,  True, False,  True],\n",
       "       [False, False, False, False],\n",
       "       [False, False, False, False]])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 50
   },
   "source": [
    "Summing all the elements in the tensor yields a tensor with only one element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "origin_pos": 52,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=66.0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 53
   },
   "source": [
    "## Broadcasting Mechanism\n",
    "\n",
    "\n",
    "In the previous section, we performed elementwise operations\n",
    "on two tensors of the same shape.\n",
    "\n",
    "Under certain conditions, even when shapes differ, we can still perform elementwise operations by invoking the *broadcasting mechanism*.\n",
    "\n",
    "This mechanism works in the following way:\n",
    "- First, expand one or both arrays\n",
    "by copying elements appropriately\n",
    "so that after this transformation,\n",
    "the two tensors have the same shape.\n",
    "- Second, carry out the elementwise operations\n",
    "on the resulting arrays.\n",
    "\n",
    "In most cases, we broadcast along an axis where an array\n",
    "initially only has length 1, such as in the following example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "origin_pos": 56,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 1), dtype=int32, numpy=\n",
       " array([[0],\n",
       "        [1],\n",
       "        [2]])>,\n",
       " <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[0, 1]])>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.reshape(tf.range(3), (3, 1))\n",
    "b = tf.reshape(tf.range(2), (1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 57
   },
   "source": [
    "Since `a` and `b` are $3\\times1$ and $1\\times2$ matrices respectively,\n",
    "their shapes do not match up if we want to add them.\n",
    "We *broadcast* the entries of both matrices into a larger $3\\times2$ matrix as follows:\n",
    "for matrix `a` it replicates the columns\n",
    "and for matrix `b` it replicates the rows\n",
    "before adding up both elementwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "origin_pos": 58,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
       "array([[0, 1],\n",
       "       [1, 2],\n",
       "       [2, 3]])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 59
   },
   "source": [
    "## Indexing and Slicing\n",
    "\n",
    "Just as in any other Python array, elements in a tensor can be accessed by index.\n",
    "As in any Python array, the first element has index 0 and ranges are specified to include the first but *before* the last element.\n",
    "\n",
    "\n",
    "As in standard Python lists, we can access elements\n",
    "according to their relative position to the end of the list\n",
    "by using negative indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 59
   },
   "source": [
    "Thus, `[-1]` selects the last element and `[1:3]`\n",
    "selects the second and the third elements as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "origin_pos": 60,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 8.,  9., 10., 11.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       " array([[ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]], dtype=float32)>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1], X[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 62,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "`Tensors` in TensorFlow are immutable, and cannot be assigned to.\n",
    "\n",
    "`Variables` in TensorFlow are mutable containers of state that support\n",
    "assignments. Keep in mind that gradients in TensorFlow do not flow backwards\n",
    "through `Variable` assignments.\n",
    "\n",
    "Beyond assigning a value to the entire `Variable`, we can write elements of a\n",
    "`Variable` by specifying indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "origin_pos": 64,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=\n",
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [ 4.,  5.,  9.,  7.],\n",
       "       [ 8.,  9., 10., 11.]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_var = tf.Variable(X)\n",
    "X_var[1, 2].assign(9)\n",
    "X_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 65
   },
   "source": [
    "If we want to assign multiple elements the same value,\n",
    "we simply index all of them and then assign them the value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 65
   },
   "source": [
    "For instance, `[0:2, :]` accesses the first and second rows,\n",
    "where `:` takes all the elements along axis 1 (column).\n",
    "\n",
    "While we discussed indexing for matrices,\n",
    "this obviously also works for vectors\n",
    "and for tensors of more than 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "origin_pos": 67,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=\n",
       "array([[12., 12., 12., 12.],\n",
       "       [12., 12., 12., 12.],\n",
       "       [ 8.,  9., 10., 11.]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_var = tf.Variable(X)\n",
    "X_var[0:2, :].assign(tf.ones(X_var[0:2, :].shape, dtype=tf.float32) * 12)\n",
    "X_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 68
   },
   "source": [
    "## Saving Memory\n",
    "\n",
    "Running operations can cause new memory to be\n",
    "allocated to host results.\n",
    "\n",
    "\n",
    "For example, if we write `Y = X + Y`,\n",
    "we will dereference the tensor that `Y` used to point to\n",
    "and instead point `Y` at the newly allocated memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 68
   },
   "source": [
    "In the following example, we demonstrate this with Python's `id()` function,\n",
    "which gives us the exact address of the referenced object in memory.\n",
    "\n",
    "After running `Y = Y + X`, we will find that `id(Y)` points to a different location.\n",
    "That is because Python first evaluates `Y + X`,\n",
    "allocating new memory for the result and then makes `Y`\n",
    "point to this new location in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "origin_pos": 69,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 70
   },
   "source": [
    "This might be undesirable for two reasons:\n",
    "- First, we do not want to run around\n",
    "allocating memory unnecessarily all the time.\n",
    "In machine learning, we might have\n",
    "hundreds of megabytes of parameters\n",
    "and update all of them multiple times per second.\n",
    "Typically, we will want to perform these updates *in place*.\n",
    "- Second, we might point at the same parameters from multiple variables.\n",
    "If we do not update in place, other references will still point to\n",
    "the old memory location, making it possible for parts of our code\n",
    "to inadvertently reference stale parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 72,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "`Variables` are mutable containers of state in TensorFlow. They provide\n",
    "a way to store your model parameters.\n",
    "We can assign the result of an operation\n",
    "to a `Variable` with `assign`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 72,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "To illustrate this concept, we create a `Variable` `Z`\n",
    "with the same shape as another tensor `Y`,\n",
    "using `zeros_like` to allocate a block of $0$ entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "origin_pos": 75,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 2330837854048\n",
      "id(Z): 2330837854048\n"
     ]
    }
   ],
   "source": [
    "Z = tf.Variable(tf.zeros_like(Y))\n",
    "print('id(Z):', id(Z))\n",
    "Z.assign(X + Y)\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 77,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "Even once you store state persistently in a `Variable`, you\n",
    "may want to reduce your memory usage further by avoiding excess\n",
    "allocations for tensors that are not your model parameters.\n",
    "\n",
    "Because TensorFlow `Tensors` are immutable and gradients do not flow through\n",
    "`Variable` assignments, TensorFlow does not provide an explicit way to run\n",
    "an individual operation in-place.\n",
    "\n",
    "However, TensorFlow provides the `tf.function` decorator to wrap computation\n",
    "inside of a TensorFlow graph that gets compiled and optimized before running.\n",
    "This allows TensorFlow to prune unused values, and to re-use\n",
    "prior allocations that are no longer needed. This minimizes the memory\n",
    "overhead of TensorFlow computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "origin_pos": 79,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 8.,  9., 26., 27.],\n",
       "       [24., 33., 42., 51.],\n",
       "       [56., 57., 58., 59.]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def computation(X, Y):\n",
    "    Z = tf.zeros_like(Y)  # This unused value will be pruned out\n",
    "    A = X + Y  # Allocations will be re-used when no longer needed\n",
    "    B = A + Y\n",
    "    C = B + Y\n",
    "    return C + Y\n",
    "\n",
    "computation(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 80
   },
   "source": [
    "## Conversion to Other Python Objects\n",
    "\n",
    "**Converting to a NumPy tensor**, or vice versa, is easy.\n",
    "The converted result does not share memory.\n",
    "\n",
    "This minor inconvenience is actually quite important:\n",
    "when you perform operations on the CPU or on GPUs,\n",
    "you do not want to halt computation, waiting to see\n",
    "whether the NumPy package of Python might want to be doing something else\n",
    "with the same chunk of memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "origin_pos": 83,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, tensorflow.python.framework.ops.EagerTensor)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = tf.constant(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 84
   },
   "source": [
    "To **convert a size-1 tensor to a Python scalar**,\n",
    "we can invoke the `item` function or Python's built-in functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "origin_pos": 87,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.5], dtype=float32), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([3.5]).numpy()\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 88
   },
   "source": [
    "## Summary\n",
    "\n",
    "* The main interface to store and manipulate data for deep learning is the tensor ($n$-dimensional array). It provides a variety of functionalities including basic mathematics operations, broadcasting, indexing, slicing, memory saving, and conversion to other Python objects.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Run the code in this section. Change the conditional statement `X == Y` in this section to `X < Y` or `X > Y`, and then see what kind of tensor you can get.\n",
    "1. Replace the two tensors that operate by element in the broadcasting mechanism with other shapes, e.g., 3-dimensional tensors. Is the result the same as expected?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 91,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "\n",
    "**Acknowledgement:** This Jupyter Notebook is adapted from the dive into Deep Learning open-source book.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
