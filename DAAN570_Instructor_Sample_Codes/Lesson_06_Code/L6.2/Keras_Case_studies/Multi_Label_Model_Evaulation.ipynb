{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BYyJ9GceOaQ-"
   },
   "source": [
    "# Multi Label Model Evaulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d3GXeEPRRYWJ"
   },
   "source": [
    "[Definitions:](https://scikit-learn.org/stable/modules/multiclass.html#multiclass-and-multilabel-algorithms)\n",
    "\n",
    "* **Multiclass classification:** classification task with more than two classes. ***Each sample can only be labelled as one class***.\n",
    "\n",
    "* For example, classification using features extracted from a set of images of fruit, ***where each image may either be of an orange, an apple, or a pear***. Each image is one sample and is labelled as ***one of the 3 possible classes***. Multiclass classification makes the assumption that each sample is assigned to **one and only one label** - one sample cannot, for example, be both a pear and an apple.\n",
    "\n",
    "\n",
    "* **Multilabel classification**: classification task labelling each sample with x labels from n_classes possible classes, where x can be **0 to n_classes** **inclusive**. This can be thought of as predicting properties of a sample that are ***not mutually exclusive***. Formally, a binary output is assigned to each class, for every sample. Positive classes are indicated with 1 and negative classes with 0 or -1. It is thus comparable to running n_classes binary classification tasks, for example with sklearn.multioutput.MultiOutputClassifier. This approach treats each label independently whereas multilabel classifiers may treat the multiple classes simultaneously, accounting for correlated behaviour amoung them.\n",
    "\n",
    "* For example, prediction of the topics relevant to a text document or video. The document or video **may be about one of** ‘religion’, ‘politics’, ‘finance’ or ‘education’, several of the topic classes or all of the topic classes.\n",
    "\n",
    "**Difference between multi-class classification & multi-label classification** is that in multi-class problems the classes are mutually exclusive, whereas for multi-label problems each label represents a different classification task, but the tasks are somehow related.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LqKZHAkgk5OY"
   },
   "source": [
    "[SciKit Learn: \n",
    "Model evaluation: quantifying the quality of predictions](https://scikit-learn.org/stable/model_selection.html?highlight=multilabel%20metrics#model-selection-and-evaluation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gW2a1OJ4NPP2"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ug67UFRlOjv"
   },
   "source": [
    "#Synthetic Data:\n",
    "[Calculate mean Average Precision (mAP) for multi-label classification](https://medium.com/@hfdtsinghua/calculate-mean-average-precision-map-for-multi-label-classification-b082679d31be)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ByXG3zppQ_B"
   },
   "source": [
    "First assume that in the data set we have only 3 samples as below. Each sample has 4 classes: A, B, C, and D. \n",
    "In samples more than one classes exist! Therefore, the problem multi-label classification!\n",
    "\n",
    "After some training, some ML model produce the predicted scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rfQJjLDmnw8m"
   },
   "source": [
    "![Sample Data with scores](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/sample%20mutli%20label%20classification%20scores.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bi_qgCxFlNeK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_true = np.array([[0, 1, 1, 1],[0,0,1,0],[1,1,0,0]])\n",
    "y_scores = np.array([[0.2, 0.6, 0.1, 0.8],[0.4,0.9,0.8,0.6],[0.8,0.4,0.5,0.7]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M4oWUGdVu6rx"
   },
   "source": [
    "#Threshold: Let's assume we are using **0.5 as the threshold** for prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oS4XSS8YvHcd"
   },
   "outputs": [],
   "source": [
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "H8WBPN1OxXmw",
    "outputId": "46106bba-2207-43e3-f7b2-fc86ea59c515"
   },
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "for sample in  y_scores:\n",
    "  y_pred.append([1 if i>=0.5 else 0 for i in sample ] )\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otI8D4rZIIvA"
   },
   "source": [
    "![Predictions](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/sample%20mutli%20label%20classification%20predictionss.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clS_XwMjvgcp"
   },
   "source": [
    "# **PART A: Basic Metrics**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JANvg0Post6n"
   },
   "source": [
    "# [Accuracy score](https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score)\n",
    "\n",
    "The accuracy_score function computes the accuracy, either the fraction (default) or the count (normalize=False) of correct predictions.\n",
    "\n",
    "**In multilabel classification**, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is **0.0**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VuegY_yhsfX7",
    "outputId": "5151583a-64aa-4f67-d2f8-465ba0e4df30"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZoikTSK1a-l"
   },
   "source": [
    "**Notice**: Here, accuracy is based on if all the 4-class prediction is correct or not.\n",
    "\n",
    "It is not label-based!\n",
    "\n",
    "Therefore accuracy is zero!\n",
    "\n",
    "We have to use better metrics for multilabel classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vV7ce4F_5PLU"
   },
   "source": [
    "# [Confusion matrix](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix) \n",
    "\n",
    "The confusion_matrix function evaluates classification accuracy by computing the confusion matrix.\n",
    "\n",
    "By definition, entry i, j in a confusion matrix is the number of observations actually in group i, but predicted to be in group j. \n",
    "\n",
    "![Sample Confusion Matrix](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/confusion%20matrix%20sample.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tyRqnM_M5OWv"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "#confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "torLpxmT7uvg"
   },
   "source": [
    "# [Multilabel confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html?highlight=multilabel#sklearn-metrics-multilabel-confusion-matrix)\n",
    "\n",
    "\n",
    "* Compute a confusion matrix for each class or sample \n",
    "* **NOTICE:** The cells has different meaning from Binary Class Confusion Matrix! \n",
    "\n",
    "![MCM](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/MCM%20confusion%20matrix%20sample.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "7JxZx_hj_K0Y",
    "outputId": "0798f554-9e96-4a4b-c598-a138c51a9045"
   },
   "outputs": [],
   "source": [
    "print(\"Actual \\n\", y_true)\n",
    "print(\"\\nPredicted \\n\",y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAj197ZtIz_5"
   },
   "source": [
    "![Predictions](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/sample%20mutli%20label%20classification%20predictionss.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "dpaqgMh71aOv",
    "outputId": "974a78d8-a4f0-4c1e-dc8a-02d73c5bb64a"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "multilabel_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I17SFfwGD8Sa"
   },
   "source": [
    "# [Classification report](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "YgaojYsXD7m-",
    "outputId": "cb3b8c7e-53a2-40b3-bfbd-a786e9950541"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "label_names = ['label A', 'label B', 'label C', 'label D']\n",
    "\n",
    "print(classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2MUfUsfyQLbQ"
   },
   "source": [
    "# [Precision, recall and F-measures](https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-and-f-measures)\n",
    "\n",
    "\n",
    "Intuitively, precision is the ability of the classifier not to label as positive a sample that is negative, and recall is the ability of the classifier to find all the positive samples.\n",
    "\n",
    "The F-measure can be interpreted as a weighted harmonic mean of the precision and recall:  reaches its best value at 1 and its worst score at 0.\n",
    "\n",
    "Several functions allow you to analyze the precision, recall and F-measures score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u4iBlAQIQven"
   },
   "source": [
    "# [Precision, recall and F-measures @ Multilabel classification ](https://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification)\n",
    "\n",
    "\n",
    "* In multiclass and multilabel classification task, the notions of precision, \n",
    "recall, and F-measures can be applied to each label independently. \n",
    "\n",
    "* There are a few ways to combine results across labels, specified by the ***average*** argument to the average_precision_score (multilabel only), f1_score, fbeta_score, precision_recall_fscore_support, precision_score and recall_score functions, as described above. \n",
    "\n",
    "* Note that for ***“micro”***-averaging in a multiclass setting with all labels included will produce equal precision, recall and F, while ***“weighted”*** averaging may produce an F-score that is not between precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9Em4uWRTCf-"
   },
   "source": [
    "## Precision\n",
    "[precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn-metrics-precision-score)\n",
    "\n",
    "\n",
    "* The precision is the ratio **tp / (tp + fp)** where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "* The best value is 1 and the worst value is 0.\n",
    "\n",
    "![Sample Confusion Matrix](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/confusion%20matrix%20sample.png?raw=true)\n",
    "\n",
    "***average*** parameter is required for multiclass/multilabel targets. \n",
    "\n",
    "* **None**, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:\n",
    "\n",
    "* '**micro**':\n",
    "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "* '**macro**':\n",
    "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "* '**weighted**':\n",
    "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "* '**samples**':\n",
    "Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "6LUSgldhTBvQ",
    "outputId": "de32c0fc-8952-4bc4-895f-3e68fff75ca0"
   },
   "outputs": [],
   "source": [
    "print(\"Actual \\n\", y_true)\n",
    "print(\"\\nPredicted \\n\",y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZK2iOH_6xHzS"
   },
   "source": [
    "####Precision of each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7SSVg4wub9qP",
    "outputId": "cc1f94da-eece-4a7f-a541-f3d5f6adacae"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(\"None \", metrics.precision_score(y_true, y_pred, average=None))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Yb2_DVhxrc5"
   },
   "source": [
    "## Average Precision\n",
    "[precision_score funtion has](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn-metrics-precision-score)\n",
    "\n",
    "***average*** parameter is required for multiclass/multilabel targets. \n",
    "\n",
    "* **None**, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:\n",
    "\n",
    "* '**micro**':\n",
    "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "* '**macro**':\n",
    "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "* '**weighted**':\n",
    "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "* '**samples**':\n",
    "Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "brRnFggmuqGB"
   },
   "source": [
    "### Use precision_score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "6voTYj5YQKuS",
    "outputId": "d1368b17-81fe-424a-bc2e-fa821b3cac52"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"micro: {:.2f}\".format(metrics.precision_score(y_true, y_pred, average='micro')))\n",
    "print(\"macro: {:.2f} \".format( metrics.precision_score(y_true, y_pred, average='macro')))\n",
    "print(\"weighted: {:.2f} \".format( metrics.precision_score(y_true, y_pred, average='weighted')))\n",
    "print(\"samples: {:.2f} \".format( metrics.precision_score(y_true, y_pred, average='samples')))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTsM9OVR3f1x"
   },
   "source": [
    "###  [Use average_precision_score function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn-metrics-average-precision-score)\n",
    "\n",
    "* average_precision_score(y_true, **y_score** , average='macro', sample_weight=None)\n",
    "\n",
    "*Compute average precision (AP) from prediction scores\n",
    "\n",
    "* average_precision_score summarizes a **precision-recall curve** as the weighted mean of precisions achieved **at each threshold**, with the increase in recall from the previous threshold used as the weight:\n",
    "\n",
    ">>$\\text{AP} = \\sum_n (R_n - R_{n-1}) P_n$\n",
    " \n",
    "where  and  are the precision and recall at the nth threshold [1]. \n",
    "* This implementation is not interpolated and is different from computing the area under the precision-recall curve with the trapezoidal rule, which uses linear interpolation and can be too optimistic.\n",
    "\n",
    "* Note: this implementation is restricted to the binary classification task or **multilabel** classification task.\n",
    "\n",
    "**IMPORTANT:** \n",
    "* Precision refers to precision at a **particular decision threshold**. For example, if you count any model output less than 0.5 as negative, and greater than 0.5 as positive. \n",
    "* But sometimes (especially if your classes are not balanced, or if you want to favor precision over recall or vice versa), you may want to **vary this threshold**. **average_precision_score function** gives you ***average precision at all such possible thresholds***, which is also similar to the ***area under the precision-recall curve***. \n",
    "* It is a useful metric to compare how well models are ordering the predictions, without considering any specific decision threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "qHEge9pB6Agn",
    "outputId": "f07c8adf-feee-47c1-c9f8-def07d5c87ce"
   },
   "outputs": [],
   "source": [
    "y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "ynqnE2p3Vf3Y",
    "outputId": "0f8896f2-54c1-4d20-d386-7feb232704c7"
   },
   "outputs": [],
   "source": [
    "print(\"micro: {:.2f}\".format(metrics.average_precision_score(y_true, y_scores, average='micro')))\n",
    "print(\"macro: {:.2f} \".format( metrics.average_precision_score(y_true, y_scores, average='macro')))\n",
    "print(\"weighted: {:.2f} \".format( metrics.average_precision_score(y_true, y_scores, average='weighted')))\n",
    "print(\"samples: {:.2f} \".format( metrics.average_precision_score(y_true, y_scores, average='samples')))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4us93CLlVjSG"
   },
   "source": [
    "### Calculate micro Precision \n",
    "* Calculate metrics **globally** by counting the total true positives and false positives.\n",
    "\n",
    "The precision is the ratio tp / (tp + fp) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tyZpgSccmqG1"
   },
   "source": [
    "**Count by yourself**\n",
    "\n",
    "![Predictions](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/sample%20mutli%20label%20classification%20predictionss.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "Ei3z0Y1Yi0b1",
    "outputId": "08c0b382-8865-4aec-b103-f4905a1ab16b"
   },
   "outputs": [],
   "source": [
    "np.concatenate((y_true.reshape((12,1), order='F'), y_pred.reshape((12,1), order='F')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "i0xJUqMQVg5E",
    "outputId": "bf17b841-d866-4046-d6af-118a01862d83"
   },
   "outputs": [],
   "source": [
    "TP=((y_true * y_pred) == 1).sum()\n",
    "print(\"TP: \", TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HCCBTjiTkHGm",
    "outputId": "6188d642-46f3-4473-ea03-e7d95417184d"
   },
   "outputs": [],
   "source": [
    "converted_y_true= np.copy(y_true)\n",
    "converted_y_true[converted_y_true==1] = 5\n",
    "converted_y_true[converted_y_true==0] = 1\n",
    "converted_y_true[converted_y_true==5] = 0\n",
    "FP= ((converted_y_true * y_pred)== 1).sum()\n",
    "print(\"FP: \", FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vUTNZw_jnaGF",
    "outputId": "a4d4015a-e1b7-4522-c5f6-c7a3c48fe9e4"
   },
   "outputs": [],
   "source": [
    "print(\" Micro Precision {:.2f}\".format(TP/(TP+FP)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NMD5-FYvmwz3"
   },
   "source": [
    "**Flatten & use as in binary classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MjXXvSElmlks",
    "outputId": "c9aab404-b2a5-41ea-ff61-75feb7ed8302"
   },
   "outputs": [],
   "source": [
    "print(\"Micro Precision: {:.2f}\".format(metrics.precision_score(y_true.ravel(), y_pred.ravel())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "rssNxBk4KgZO",
    "outputId": "6a18f93b-c303-4500-89d8-d888db8134a1"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHCVGTQBbTAN"
   },
   "source": [
    "### Calculate macro Precision \n",
    "* Calculate metrics for each label, and find their **unweighted** mean. \n",
    "* This **does not** take label **imbalance** into account.\n",
    "\n",
    "The precision is the ratio tp / (tp + fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "R3xfOUsblgWo",
    "outputId": "606df99e-6242-4343-e5c7-0e156aa7b31c"
   },
   "outputs": [],
   "source": [
    "totalPrecision= 0\n",
    "for i in range (len(label_names)):\n",
    "  p= metrics.precision_score(y_true[:,i], y_pred[:,i])\n",
    "  totalPrecision+= p\n",
    "  print(\"For {} precision: {:.2f}\".format(label_names[i], p))\n",
    "print(\"Macro Precision: {:.2f}\".format(totalPrecision/len(label_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J9ksrGUnqFG4"
   },
   "source": [
    "### Calculate weighted Precision \n",
    "*  Calculate metrics for each label, and find their average **weighted by support** (the number of true instances for each label). \n",
    "*  This alters ‘macro’ to account for label **imbalance**; it can result in an *F-score that is not between precision and recall*.\n",
    "\n",
    "The precision is the ratio tp / (tp + fp) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "v8f52g6-qix9",
    "outputId": "221bd1d9-0261-401c-f596-0fb1b4fb6f7b"
   },
   "outputs": [],
   "source": [
    "totalPrecision=0\n",
    "totalSupport=0\n",
    "for i in range (len(label_names)):\n",
    "  p= metrics.precision_score(y_true[:,i], y_pred[:,i])\n",
    "  support= (y_true[:,i]==1).sum()\n",
    "  totalSupport+=support\n",
    "  totalPrecision+= p*support\n",
    "  print(\"For {} precision: {:.2f} support: {}\".format(label_names[i], p, support ))\n",
    "print(\"Weighted Precision: {:.2f}\".format(totalPrecision/totalSupport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "KW_1Q4kFcjXG",
    "outputId": "630acead-b691-4fc0-f7ee-7b2530162510"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQ6Sqfj5tkYC"
   },
   "source": [
    "### Calculate Samples Precision \n",
    "\n",
    "* Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score).\n",
    "\n",
    "![Predictions](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/sample%20mutli%20label%20classification%20predictionss.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "-uPoPdhubOtr",
    "outputId": "9911a5e9-3de2-42c6-d328-e7a308f3262d"
   },
   "outputs": [],
   "source": [
    "totalPrecision=0\n",
    "\n",
    "for i in range (len(y_true)):\n",
    "  p= metrics.precision_score(y_true[i,:], y_pred[i,:])\n",
    "  totalPrecision+=p\n",
    "  print(\"For Sample {} precision: {:.2f} \".format(y_true[i,:], p ))\n",
    "print(\"Sample Precision: {:.2f}\".format(totalPrecision/len(y_true)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ex9ejpyech_b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pf9zjgAuyy8Z"
   },
   "source": [
    "## Recall\n",
    "[recall_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html?highlight=recall_score#sklearn-metrics-recall-score)\n",
    "\n",
    "\n",
    "* Compute the recall: The recall is the ratio **tp / (tp + fn)** where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "* The best value is 1 and the worst value is 0.\n",
    "\n",
    "![Sample Confusion Matrix](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/confusion%20matrix%20sample.png?raw=true)\n",
    "\n",
    "\n",
    "***average*** parameter is required for multiclass/multilabel targets. \n",
    "\n",
    "* **None**, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:\n",
    "\n",
    "* '**micro**':\n",
    "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "* '**macro**':\n",
    "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "* '**weighted**':\n",
    "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "* '**samples**':\n",
    "Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VC4olLOAzzcW"
   },
   "source": [
    "### Recall of each label\n",
    "\n",
    "![Predictions](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/sample%20mutli%20label%20classification%20predictionss.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "abwQfDgcz7ua",
    "outputId": "d34299a8-6788-4740-c7f5-0ad3a7c4cd71"
   },
   "outputs": [],
   "source": [
    "print(\"Recall of each label: {}\".format(metrics.recall_score(y_true, y_pred, average=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhwKB72T0ObO"
   },
   "source": [
    "### Average Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "sRYt0RY-TAHr",
    "outputId": "ee4df5fe-3dc5-4e4a-f5d1-40940aff4df2"
   },
   "outputs": [],
   "source": [
    "print(\"micro: {:.2f}\".format(metrics.recall_score(y_true, y_pred, average='micro')))\n",
    "print(\"macro: {:.2f} \".format( metrics.recall_score(y_true, y_pred, average='macro')))\n",
    "print(\"weighted: {:.2f} \".format( metrics.recall_score(y_true, y_pred, average='weighted')))\n",
    "print(\"samples: {:.2f} \".format( metrics.recall_score(y_true, y_pred, average='samples')))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MUkjouJZ1DVo"
   },
   "source": [
    "##F1 score\n",
    "[f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html?highlight=f1_score#sklearn-metrics-f1-score)\n",
    "\n",
    "\n",
    "* Compute the F1 score, also known as balanced F-score or F-measure\n",
    "\n",
    "* The F1 score can be interpreted as a ***weighted average of the precision and recall***, where an F1 score reaches its best value at 1 and worst score at 0. \n",
    "\n",
    "* The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "\n",
    "> >>`F1 = 2 * (precision * recall) / (precision + recall)`\n",
    "\n",
    "\n",
    "In the multi-class and **multi-label** case, this is the** weighted average of the F1 score of each class**.\n",
    "\n",
    "\n",
    "***average*** parameter is required for multiclass/multilabel targets. \n",
    "\n",
    "* **None**, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:\n",
    "\n",
    "* '**micro**':\n",
    "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "* '**macro**':\n",
    "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "* '**weighted**':\n",
    "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8nvFtRZ52LqM"
   },
   "source": [
    "### F1 of each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6ZyJhPtk2I8C",
    "outputId": "ec399a76-6023-4229-8b4b-6807db27d1cb"
   },
   "outputs": [],
   "source": [
    "print(\"F1 of each label: {}\".format(metrics.f1_score(y_true, y_pred, average=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UB9zCQZ_2ZtA"
   },
   "source": [
    "### Average F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "ngMnEJn22Knv",
    "outputId": "b1a343ed-ba75-4826-95f4-f837afa9a0dc"
   },
   "outputs": [],
   "source": [
    "print(\"micro: {:.2f}\".format(metrics.f1_score(y_true, y_pred, average='micro')))\n",
    "print(\"macro: {:.2f} \".format( metrics.f1_score(y_true, y_pred, average='macro')))\n",
    "print(\"weighted: {:.2f} \".format( metrics.f1_score(y_true, y_pred, average='weighted')))\n",
    "print(\"samples: {:.2f} \".format( metrics.f1_score(y_true, y_pred, average='samples')))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dyiQ7VWrA0jR"
   },
   "source": [
    "## Summary\n",
    "\n",
    "* **Remember**: We assumed that we are using **0.5 as the threshold** for prediction during all the computations above.\n",
    "\n",
    "* What hapens if we select **different** values as the threshold?\n",
    "\n",
    "Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "dwdlkbtHBwwS",
    "outputId": "2540b32b-e1ac-4f67-e571-a958cd7869a1"
   },
   "outputs": [],
   "source": [
    "print(\"y_true: \\n{}\".format(y_true))\n",
    "print(\"y_scores: \\n{}\".format(y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QUr9pSLGCG2o"
   },
   "outputs": [],
   "source": [
    "def predictLabelForGivenThreshold(threshold):\n",
    "  y_pred=[]\n",
    "  for sample in  y_scores:\n",
    "    y_pred.append([1 if i>=threshold else 0 for i in sample ] )\n",
    "  return np.array(y_pred)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ycJwx8OrDAl6"
   },
   "source": [
    "### Classification Report when threshold = 0.5\n",
    "\n",
    "![Predictions](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/sample%20mutli%20label%20classification%20predictionss.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "K6-f50NTCfjq",
    "outputId": "391e2c3a-b1c3-41d7-95a2-5b61bb08040c"
   },
   "outputs": [],
   "source": [
    "y_pred  = predictLabelForGivenThreshold(0.5)\n",
    "print(classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u7QQURxEDKgn"
   },
   "source": [
    "### Classification Report when threshold = 0.2\n",
    "\n",
    "![alt text](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/sample%20mutli%20label%20classification%20predictions%2002.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "sFHVqB6fDJyA",
    "outputId": "1f367a84-a578-4a20-fafb-db784d39f85a"
   },
   "outputs": [],
   "source": [
    "y_pred  = predictLabelForGivenThreshold(0.2)\n",
    "print(classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-F6hT-RzDNmf"
   },
   "source": [
    "### Classification Report when threshold = 0.8\n",
    "\n",
    "![alt text](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/sample%20mutli%20label%20classification%20predictions%2008.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "2iLWmsHeDTRX",
    "outputId": "0959d06f-7b5f-4a10-b893-4aff7304cbea"
   },
   "outputs": [],
   "source": [
    "y_pred  = predictLabelForGivenThreshold(0.8)\n",
    "print(classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zLTSn1q6D184"
   },
   "source": [
    "### Which threshold?\n",
    "\n",
    "* Which threshold yileds better result?\n",
    "* Which metric do you care?\n",
    "* How can you compare different models' success/performance?\n",
    "* Which precision/recall/f1 value to report?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9b7Ef39k0PD3"
   },
   "source": [
    "<img src=\"https://github.com/kmkarakaya/ML_tutorials/blob/master/images/ROC.png?raw=true\" alt=\"drawing\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wbT1zZDNvTmc"
   },
   "source": [
    "# PART B: ROC & AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9eN_obQ8wHvB"
   },
   "source": [
    "# [Receiver operating characteristic (ROC) ](https://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc)\n",
    "\n",
    "The function **roc_curve** computes the **receiver operating characteristic** curve, or ROC curve. \n",
    "\n",
    "Quoting Wikipedia :\n",
    "* A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the ***performance of a binary classifier*** system as its discrimination **threshold is varied**. \n",
    "\n",
    "![Sample Confusion Matrix](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/confusion%20matrix%20sample.png?raw=true)\n",
    "\n",
    "\n",
    "* It is created by plotting the fraction of ***true positives out of the positives*** (TPR = true positive rate) vs. the fraction of ***false positives out of the negatives*** (FPR = false positive rate), at **various threshold settings**. \n",
    "\n",
    "<img src=\"https://github.com/kmkarakaya/ML_tutorials/blob/master/images/ROC.png?raw=true\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "\n",
    "* TPR is also known as **sensitivity**, and FPR is one minus the **specificity** or true negative rate.”\n",
    "\n",
    "\n",
    "* The **roc_auc_score function** computes the ***area*** under the receiver operating characteristic (ROC) curve, which is also denoted by AUC or AUROC. By computing the area under the roc curve, **the curve information is summarized in one number**. \n",
    "\n",
    "* In **multi-label** classification, the roc_auc_score function is extended by ***averaging over the labels*** as above.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w26dgZXKzhJN"
   },
   "source": [
    "## [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html?highlight=roc_curve#sklearn-metrics-roc-curve)\n",
    "\n",
    "Compute Receiver operating characteristic (ROC)\n",
    "\n",
    "* Note: this implementation is restricted to the **binary classification** task.\n",
    "\n",
    "\n",
    "Returns:\n",
    "\n",
    ">* **fpr**: Increasing false positive rates such that element i is the false positive rate of predictions with score >= thresholds[i].\n",
    "\n",
    ">* **tpr**: Increasing true positive rates such that element i is the true positive rate of predictions with score >= thresholds[i].\n",
    "\n",
    ">* **thresholds**: Decreasing thresholds on the decision function used to compute fpr and tpr. thresholds[0] represents no instances being predicted and is arbitrarily set to max(y_score) + 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xN-utxJGSVkZ"
   },
   "source": [
    "### One label at a time\n",
    "\n",
    "* We can NOT directly use roc_curve funtion to **multilabel** classification.\n",
    "\n",
    "* However, we can select 1 label at a time and calculate its roc_curve values!\n",
    "\n",
    "* Remeber we are given that:\n",
    "\n",
    "![Sample Data with scores](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/sample%20mutli%20label%20classification%20scores.png?raw=true)\n",
    "\n",
    "\n",
    "### Classification Report when threshold = 0.5\n",
    "\n",
    "![Predictions](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/sample%20mutli%20label%20classification%20predictionss.png?raw=true)\n",
    "\n",
    "\n",
    "### Classification Report when threshold = 0.2\n",
    "\n",
    "![alt text](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/sample%20mutli%20label%20classification%20predictions%2002.png?raw=true)\n",
    "\n",
    "### Classification Report when threshold = 0.8\n",
    "\n",
    "![alt text](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/sample%20mutli%20label%20classification%20predictions%2008.png?raw=true)\n",
    "\n",
    "\n",
    "**For eachthreshold we will end up with a different Confusion Matrix. For example for threshold 0.8 for Label A we can fill the following Confusion Matrix as below:\n",
    "\n",
    "![Sample Confusion Matrix](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/confusion%20matrix%20sample.png?raw=true)\n",
    "\n",
    "\n",
    "* Let's select first label as a binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SgSw3_lXXKRt"
   },
   "outputs": [],
   "source": [
    "#@title Let's select first label as a binary classification:\n",
    "selectedLabel=2 #@param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "QewwxvkKQYsj",
    "outputId": "55f152e3-f9f2-4a38-a842-95798e7b597d"
   },
   "outputs": [],
   "source": [
    "print(\"y_true\\n\", y_true)\n",
    "y_binary = y_true[:,selectedLabel]\n",
    "print(\"y_binary\\n\", y_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "j5tEKT2_Q1XU",
    "outputId": "09471540-362c-4caf-9c4a-acdda5654c06"
   },
   "outputs": [],
   "source": [
    "y_binary_scores = y_scores[:,selectedLabel]\n",
    "print(\"y_binary_scores\\n\", y_binary_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-A0f_sxNT9K"
   },
   "source": [
    "#### Finding TPR, FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "VwKU41qlyRlI",
    "outputId": "2f9af288-fd0a-4471-b556-27acc43b72bf"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_binary, y_binary_scores)\n",
    "print(\"selected label {}\\nfpr {} \\ntpr {} \\nthresholds {}\".format(label_names[selectedLabel], fpr, tpr, thresholds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WCy_nFHkMPhT"
   },
   "source": [
    "<img src=\"https://github.com/kmkarakaya/ML_tutorials/blob/master/images/sample%20mutli%20label%20classification%20scores.png?raw=true\" alt=\"sample\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "AL-DtKpFObrk",
    "outputId": "2cf54282-c364-48e0-996e-634f429f9afb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = ?)' )\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for {}'.format(label_names[selectedLabel]))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pUsdK27-0Xa6"
   },
   "source": [
    "## [plot_roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html?highlight=roc_curve#sklearn-metrics-plot-roc-curve)\n",
    "\n",
    "Note: this implementation is restricted to the **binary classification** task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Al_wOWgRN1n2"
   },
   "source": [
    "### Let's create a Syntectic multi Label dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jwBfwhHuJltF",
    "outputId": "6f10961a-47fa-4604-ba1f-e23f1736a7c1"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from scikitplot.metrics import plot_roc_curve\n",
    "import scikitplot as skplt\n",
    "\n",
    "X, y = make_multilabel_classification(n_classes=5, n_labels=3, allow_unlabeled=False, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "print(\"X_train {} y_train {} \".format(X_train.shape,y_train.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-cBlzqdgOYAS"
   },
   "source": [
    "### Train a OneVsRestClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "colab_type": "code",
    "id": "ZCxKU4R0dQON",
    "outputId": "d5decd9a-3c1f-4ea2-abbe-2d00b8a81263"
   },
   "outputs": [],
   "source": [
    "clf = OneVsRestClassifier(LogisticRegression(random_state=0))\n",
    "clf.fit(X_train, y_train)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGu0EpgnOlYT"
   },
   "source": [
    "### Get the predictions & scores from the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F0_lTtesGF9P"
   },
   "outputs": [],
   "source": [
    "preds = clf.predict(X_test )\n",
    "scores = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "IfU_nONdDW9B",
    "outputId": "af4ca5a8-3ceb-4cbe-80c1-00426b68a905"
   },
   "outputs": [],
   "source": [
    "sample=15\n",
    "print(\"prediction for sample {} {}\".format(sample, preds[sample]))\n",
    "print(\"probabilities for sample {} {}\".format(sample, scores[sample]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M5eUMbMkrV6n"
   },
   "source": [
    "### In a multilabel classification, plot_roc_curve will **generate error!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t68r2JEjGAVe"
   },
   "outputs": [],
   "source": [
    "#plot_roc_curve(clf, X_test, y_test)  \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "iDUb2XSFd00F"
   },
   "source": [
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-107-d9b794d228ad> in <module>()\n",
    "----> 1 plot_roc_curve(clf, X_test, y_test)\n",
    "      2 plt.show()\n",
    "\n",
    "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_plot/roc_curve.py in plot_roc_curve(estimator, X, y, sample_weight, drop_intermediate, response_method, name, ax, **kwargs)\n",
    "    178     if y_pred.ndim != 1:\n",
    "    179         if y_pred.shape[1] != 2:\n",
    "--> 180             raise ValueError(classification_error)\n",
    "    181         else:\n",
    "    182             y_pred = y_pred[:, 1]\n",
    "\n",
    "**ValueError: OneVsRestClassifier should be a binary classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dniXq9Gnua9"
   },
   "source": [
    "### One Label at a time: A simple work around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "YoO7xPTUgp6x",
    "outputId": "03c7a17a-9fcd-4ed0-9795-812bf883f8d5"
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train[:,0])\n",
    "\n",
    "label_0=cikitplot.metrics.plot_roc(clf, X_test, y_test[:,0], name= 'label 0',)  \n",
    "\n",
    "\n",
    "#label_0= plot_roc_curve(clf, X_test, y_test[:,0], name= 'label 0',)  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "F2sIAdfBjV0v",
    "outputId": "d590ba7e-2288-4ad5-97fb-ec59fcba486e"
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train[:,1])\n",
    "ax = plt.gca()\n",
    "label_1 = plot_roc_curve(clf, X_test, y_test[:,1], name= 'label 1', ax=ax, alpha=0.8)\n",
    "label_0.plot(ax=ax, alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dfyjnoenn55R"
   },
   "source": [
    "### In a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "qXMANP5qlcw4",
    "outputId": "27c512db-85a4-49d5-d36a-f06aaab1d97e"
   },
   "outputs": [],
   "source": [
    "labelPlots ={}\n",
    "for i in range (len(label_names)+1):\n",
    "  clf = LogisticRegression(random_state=0)\n",
    "  clf.fit(X_train, y_train[:,i])\n",
    "  ax = plt.gca()\n",
    "  labelPlots[i]= plot_roc_curve(clf, X_test, y_test[:,i], name= ('label_'+str(i)), ax=ax, alpha=0.8) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XxQcAouOIs2y"
   },
   "source": [
    "<img src=\"https://github.com/kmkarakaya/ML_tutorials/blob/master/images/ROC.png?raw=true\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-T6gNq8m1WM9"
   },
   "source": [
    "## [roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn-metrics-roc-auc-score)\n",
    "\n",
    "Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "\n",
    "Note: **this implementation can be used** with binary, multiclass and **multilabel classification**, but some restrictions apply (see Parameters).\n",
    "\n",
    "roc_auc_score(y_true, y_score, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)\n",
    "\n",
    "Parameters:\n",
    "\n",
    "* **y_true** array-like of shape (n_samples,) or (n_samples, n_classes)\n",
    "True labels or binary label indicators. The binary and multiclass cases expect labels with shape (n_samples,) while the **multilabel case expects binary label indicators with shape (n_samples, n_classes)**.\n",
    "\n",
    "* **y_score**: array-like of shape (n_samples,) or (n_samples, n_classes)\n",
    "Target scores. In the binary and multilabel cases, these can be either probability estimates or non-thresholded decision values (as returned by decision_function on some classifiers). **In the multiclass** case, these must be **probability estimates which sum to 1**. The binary case expects a shape (n_samples,), and the scores must be the scores of the class with the greater label. The multiclass and **multilabel cases expect a shape (n_samples, n_classes). In the multiclass case, the order of the class scores must correspond to the order of labels, if provided, or else to the numerical or lexicographical order of the labels in y_true.**\n",
    "\n",
    "* **average**: {‘micro’, ‘macro’, ‘samples’, ‘weighted’} or None, default=’macro’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "DTrAlG9Z2hs9",
    "outputId": "ae57787f-9360-43c5-ccd9-0e1c29727e25"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"roc_auc_score for different averaging methods:\")\n",
    "print(\"\\tmacro:{:.2} \".format(roc_auc_score(y_test, scores)))\n",
    "print(\"\\tmicro: {:.2} \".format(roc_auc_score(y_test, scores, average='micro')))\n",
    "print(\"\\tweighted: {:.2} \".format(roc_auc_score(y_test, scores, average='weighted')))\n",
    "print(\"\\tNone: {} \".format(roc_auc_score(y_test, scores, average=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D04B-F2CGsZc",
    "outputId": "e2f2b7f2-2d8c-4df5-8c63-5b7a5444f3be"
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, scores, average=None).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y9U5_X4NNPP8"
   },
   "source": [
    "# PART C: Precision-Recall Curve\n",
    "\n",
    "\n",
    "Precision-Recall is a useful measure of success of prediction when the\n",
    "classes are very **imbalanced**. In information retrieval, precision is a\n",
    "measure of result relevancy, while recall is a measure of how many truly\n",
    "relevant results are returned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8Om2ItPKUip"
   },
   "source": [
    "## IMPLICATIONS:\n",
    "* The precision-recall curve shows the **tradeoff** between precision and\n",
    "recall for different threshold. \n",
    "\n",
    "* A **high area** under the curve represents\n",
    "both **high** **recall** and high **precision**.\n",
    "\n",
    "* A **high precision** relates to a **low false positive rate**\n",
    "\n",
    "* A **high recall** relates to a **low false negative rate**. \n",
    "\n",
    "* **High scores for both** show that the classifier is returning **accurate results** (high precision), as well as returning a **majority of all positive results** (high recall).\n",
    "\n",
    "* A system with **high recall but low precision** returns **many results**, but **most** of its predicted labels are **incorrect** when compared to the training labels. \n",
    "\n",
    "* A system with **high precision but low recall** is just the opposite, returning **very few results**, but **most** of its predicted labels are **correct** when compared to the\n",
    "training labels. \n",
    "\n",
    "* An **ideal system** with **high precision** and **high recall** will return **many results**, with all results labeled **correctly**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CY9N5L8cKV_D"
   },
   "source": [
    "## DEFINITIONS\n",
    "\n",
    "* **Precision** **($P$)** is defined as the number of true positives ($T_p$) over the number of true positives plus the number of false positives ($F_p$)\n",
    "$P = \\frac{T_p}{T_p+F_p}$\n",
    "\n",
    "* **Recall ($R$)** is defined as the number of true positives ($T_p$) over the number of true positives plus the number of false negatives ($F_n$).\n",
    "$R = \\frac{T_p}{T_p + F_n}$\n",
    "\n",
    "* **($F_1$) score** These quantities are also related to the **($F_1$) score**, which is defined as the harmonic mean of precision and recall.\n",
    "$F1 = 2\\frac{P \\times R}{P+R}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6rfWLkUjLuYl"
   },
   "source": [
    "## INSIGHT:\n",
    "* Note that the **precision** **may not** **decrease** with **recall**. \n",
    "* The definition of **precision** ($\\frac{T_p}{T_p + F_p}$) shows that **lowering** the **threshold** of a classifier **may increase** the denominator, by increasing the\n",
    "number of results returned. \n",
    "\n",
    "* If the threshold was **previously set too high**, the\n",
    "new results may all be true positives, which will **increase precision**. \n",
    "\n",
    "* If the previous threshold was **about right or too low**, further lowering the threshold will introduce false positives, **decreasing precision**.\n",
    "\n",
    "* Recall is defined as $\\frac{T_p}{T_p+F_n}$, where $T_p+F_n$ **does not depend** on the classifier **threshold**. \n",
    "\n",
    "* This means that **lowering** the classifier threshold **may increase recall**, by increasing the number of true positive\n",
    "results. \n",
    "\n",
    "* It is also possible that **lowering** the threshold may leave recall **unchanged**, while the **precision fluctuates**.\n",
    "\n",
    "* The relationship between recall and precision can be observed in the **stairstep** area of the plot - **at the edges** of these steps a **small change** in the threshold considerably **reduces precision**, with only a **minor gain in recall**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y59WG9J0OODt"
   },
   "source": [
    "**Average precision** (AP) summarizes such a plot as the weighted mean of\n",
    "precisions achieved at each threshold, with the increase in recall from the\n",
    "previous threshold used as the weight:\n",
    "\n",
    "$\\text{AP} = \\sum_n (R_n - R_{n-1}) P_n$\n",
    "\n",
    "where $P_n$ and $R_n$ are the precision and recall at the\n",
    "nth threshold. A pair $(R_k, P_k)$ is referred to as an\n",
    "*operating point*.\n",
    "\n",
    "AP and the trapezoidal area under the operating points\n",
    "(:func:`sklearn.metrics.auc`) are common ways to summarize a precision-recall\n",
    "curve that lead to different results. Read more in the\n",
    "`User Guide <precision_recall_f_measure_metrics>`.\n",
    "\n",
    "Precision-recall curves are typically used in binary classification to study\n",
    "the output of a classifier. In order to extend the precision-recall curve and\n",
    "average precision to multi-class or multi-label classification, it is necessary\n",
    "to binarize the output. One curve can be drawn per label, but one can also draw\n",
    "a precision-recall curve by considering each element of the label indicator\n",
    "matrix as a binary prediction (micro-averaging).\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>See also :func:`sklearn.metrics.average_precision_score`,\n",
    "             :func:`sklearn.metrics.recall_score`,\n",
    "             :func:`sklearn.metrics.precision_score`,\n",
    "             :func:`sklearn.metrics.f1_score`</p></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r8tgsUdJNPP8"
   },
   "source": [
    "## In binary classification settings\n",
    "--------------------------------------------------------\n",
    "\n",
    "__Create simple data__\n",
    "\n",
    "Try to differentiate the two first classes of the iris data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gkoZ5aY9NPP9"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZlpMitE0OMuU",
    "outputId": "812afacf-1dff-44d9-a432-b2006c644658"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NE0n9UwJOFas"
   },
   "outputs": [],
   "source": [
    "# Add noisy features\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "random_features= random_state.randn(n_samples, 200 * n_features)\n",
    "X = np.c_[X,random_features ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vxsYP0BlPLGP",
    "outputId": "18651e72-dbf5-4101-a961-2f42bb5d02b4"
   },
   "outputs": [],
   "source": [
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OoGNcwRKPLP5"
   },
   "outputs": [],
   "source": [
    "# Limit to the two first classes, and split into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[y < 2], y[y < 2],\n",
    "                                                    test_size=.5,\n",
    "                                                    random_state=random_state)\n",
    "\n",
    "# Create a simple classifier\n",
    "classifier = svm.LinearSVC(random_state=random_state)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_score = classifier.decision_function(X_test)\n",
    "y_predict = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "4cDvVXsQRrC_",
    "outputId": "dc779b10-b076-4fd8-f708-5cad434d2fc1"
   },
   "outputs": [],
   "source": [
    "print(y_score[:5])\n",
    "print(y_predict[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A8LcyCN-NPQA"
   },
   "source": [
    "__Compute the average precision score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HgNxMWSeNPQA",
    "outputId": "f888b6b1-761e-446d-feda-5da7b5486244"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(y_test, y_score)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ULSdr9zyWDQR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "orOUpeLpNPQD"
   },
   "source": [
    "__Plot the Precision-Recall curve__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "QafOhGtcNPQE",
    "outputId": "f1e2e3bb-ea28-4483-8c5b-886ea90426c8"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "disp = plot_precision_recall_curve(classifier, X_test, y_test)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pEkdc26rNPQH"
   },
   "source": [
    "## In multi-label settings\n",
    "------------------------\n",
    "\n",
    "__Create multi-label data, fit, and predict__\n",
    "\n",
    "We create a multi-label dataset, to illustrate the precision-recall in\n",
    "multi-label settings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JBmgrtOqNPQI"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Use label_binarize to be multi-label like settings\n",
    "Y = label_binarize(y, classes=[0, 1, 2])\n",
    "n_classes = Y.shape[1]\n",
    "\n",
    "# Split into training and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5,\n",
    "                                                    random_state=random_state)\n",
    "\n",
    "# We use OneVsRestClassifier for multi-label prediction\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Run classifier\n",
    "classifier = OneVsRestClassifier(svm.LinearSVC(random_state=random_state))\n",
    "classifier.fit(X_train, Y_train)\n",
    "y_score = classifier.decision_function(X_test)\n",
    "y_predict =  classifier.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "colab_type": "code",
    "id": "xSQm_FT-N0wW",
    "outputId": "cf066939-a128-4994-93d3-dcf2de6bd62a"
   },
   "outputs": [],
   "source": [
    "print(y_score[:5])\n",
    "print(y_predict[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qutFjTSlNPQM"
   },
   "source": [
    "__The average precision score in multi-label settings__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WTwn_6oiNPQM",
    "outputId": "0527b2d4-07d9-4e1b-cf33-d6f2b440fd31"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i],\n",
    "                                                        y_score[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(),\n",
    "    y_score.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test, y_score,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "      .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TWArVpUBfE0B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "79AOqlKgfFKE"
   },
   "source": [
    "### Synthetic Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "0UPLTag3zGvP",
    "outputId": "4f8bac4c-7f87-4574-8fce-62056fbae4b5"
   },
   "outputs": [],
   "source": [
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_true, y_scores)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "9beRh8mw3qK6",
    "outputId": "26b9c6cc-ee82-49cc-a926-2e9a8a6417f8"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "#plt.step(np.append(0.0,thresholds), np.flip(precision), where='post')\n",
    "plt.step(np.append(0.0,thresholds), np.flip(recall), where='post')\n",
    "plt.xlabel('Treshold')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(' precision  vs threshold ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t58nvO7nIJPE"
   },
   "outputs": [],
   "source": [
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "treshold = dict()\n",
    "average_precision = dict()\n",
    "\n",
    "\n",
    "Y_actual= np.array([[0,1,1,1], [0,0,1,0], [1,1,0,0]], dtype=np.int16)\n",
    "y_prediction= np.array([[0.2,0.6,0.1,0.8],[0.4,0.9,0.8,0.6],[0.8,0.4,0.5,0.7]], dtype=np.float64)\n",
    "\n",
    "n_classes = Y_actual.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qADJPjQOlTc_",
    "outputId": "b6894df9-f8f7-4a3b-9309-cc068af15f8d"
   },
   "outputs": [],
   "source": [
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], treshold[i] = precision_recall_curve(Y_actual[:, i], y_prediction[:, i])\n",
    "    average_precision[i] =average_precision_score(Y_actual[:, i], y_prediction[:, i])\n",
    "\n",
    "print (\"average_precision {} \".format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "-dyoTmqZqD0a",
    "outputId": "18234a71-116c-4f59-b9b3-c9718d244aef"
   },
   "outputs": [],
   "source": [
    "x=np.append(treshold[2],1)\n",
    "\n",
    "print(x)\n",
    "print((precision[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "cYHXbYbwnHfR",
    "outputId": "75966054-3967-409f-93ad-e742d4ad782e"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for i in range(n_classes):\n",
    "  plt.step(np.append(treshold[i],1), precision[i], where='post')\n",
    "  plt.xlabel('Treshold')\n",
    "  plt.ylabel('Precision')\n",
    "  plt.ylim([0.0, 1.05])\n",
    "  plt.xlim([0.0, 1.0])\n",
    "  plt.title(\n",
    "    'Average precision score for class {}: AP={:.3f}'.format(i, average_precision[i]))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "WFxSX_tRlYKv",
    "outputId": "978a04c4-7ec0-4ac3-cf3c-1cad85304b51"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for i in range(n_classes):\n",
    "  plt.step(recall[i], precision[i], where='post')\n",
    "  plt.xlabel('Recall')\n",
    "  plt.ylabel('Precision')\n",
    "  plt.ylim([0.0, 1.05])\n",
    "  plt.xlim([0.0, 1.0])\n",
    "  plt.title(\n",
    "    'Average precision score for class {}: AP={:.3f}'.format(i, average_precision[i]))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "colab_type": "code",
    "id": "oaKrbEcdaqDJ",
    "outputId": "dbd72f14-cc59-4631-99ae-138f2cadc550"
   },
   "outputs": [],
   "source": [
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_actual.ravel(),\n",
    "    y_prediction.ravel())\n",
    "average_precision[\"None\"] = average_precision_score(Y_actual, y_prediction, average=None)\n",
    "average_precision[\"Void\"] = average_precision_score(Y_actual, y_prediction)\n",
    "average_precision[\"micro\"] = average_precision_score(Y_actual, y_prediction, average=\"micro\")\n",
    "average_precision[\"macro\"] = average_precision_score(Y_actual, y_prediction, average=\"macro\")\n",
    "average_precision[\"samples\"] = average_precision_score(Y_actual, y_prediction, average=\"samples\")\n",
    "average_precision[\"weighted\"] = average_precision_score(Y_actual, y_prediction, average=\"weighted\")\n",
    "\n",
    "#print('Average precision score, micro-averaged over all classes: {0:0.2f}'.format(average_precision[\"micro\"]))\n",
    "    \n",
    "average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "63DZQQA8P770",
    "outputId": "684e684f-b5f5-4dd1-b23b-31976e619e4e"
   },
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QXVi6JmuNPQQ"
   },
   "source": [
    "__Plot the micro-averaged Precision-Recall curve__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "G_hWUHgvNPQQ",
    "outputId": "f5b575aa-ceac-4fdb-8849-f5913130827c"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.step(recall['micro'], precision['micro'], where='post')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\n",
    "    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n",
    "    .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TgYN9E2JNPQT"
   },
   "source": [
    "__Plot Precision-Recall curve for each class and iso-f1 curves__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "colab_type": "code",
    "id": "M-ymEilJ_I3s",
    "outputId": "e5fc605f-b0b4-467f-da31-b7df2d98a303"
   },
   "outputs": [],
   "source": [
    "np.linspace(0.01, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "id": "Gmyw68IwNPQU",
    "outputId": "5906df29-d932-4b23-cef3-bac4a1a8c572"
   },
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "# setup plot details\n",
    "colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(7, 8))\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.2f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Acknowledgement:** \n",
    "All credits go to [Professor Murat Karakaya](https://www.muratkarakaya.net) and his excellent contributions of this series of deep learning tutorials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Multi Label Model Evaulation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
