{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Gradient Descent\n",
    "\n",
    "## One-Dimensional Gradient Descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from dl import tensorflow as dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 4,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def f(x):  # Objective function\n",
    "    return x**2\n",
    "\n",
    "def f_grad(x):  # Gradient (derivative) of the objective function\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def gd(eta, f_grad):\n",
    "    x = 10.0\n",
    "    results = [x]\n",
    "    for i in range(10):\n",
    "        x -= eta * f_grad(x)\n",
    "        results.append(float(x))\n",
    "    print(f'epoch 10, x: {x:f}')\n",
    "    return results\n",
    "\n",
    "results = gd(0.2, f_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 8,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def show_trace(results, f):\n",
    "    n = max(abs(min(results)), abs(max(results)))\n",
    "    f_line = tf.range(-n, n, 0.01)\n",
    "    dl.set_figsize()\n",
    "    dl.plot([f_line, results],\n",
    "             [[f(x) for x in f_line], [f(x) for x in results]], 'x', 'f(x)',\n",
    "             fmts=['-', '-o'])\n",
    "\n",
    "show_trace(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "### Learning Rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "show_trace(gd(0.05, f_grad), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "show_trace(gd(1.1, f_grad), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "### Local Minima\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "c = tf.constant(0.15 * np.pi)\n",
    "\n",
    "def f(x):  # Objective function\n",
    "    return x * tf.cos(c * x)\n",
    "\n",
    "def f_grad(x):  # Gradient of the objective function\n",
    "    return tf.cos(c * x) - c * x * tf.sin(c * x)\n",
    "\n",
    "show_trace(gd(2, f_grad), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "## Multivariate Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 16,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def train_2d(trainer, steps=20, f_grad=None): \n",
    "    \"\"\"Optimize a 2D objective function with a customized trainer.\"\"\"\n",
    "    # `s1` and `s2` are internal state variables that will be used later\n",
    "    x1, x2, s1, s2 = -5, -2, 0, 0\n",
    "    results = [(x1, x2)]\n",
    "    for i in range(steps):\n",
    "        if f_grad:\n",
    "            x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)\n",
    "        else:\n",
    "            x1, x2, s1, s2 = trainer(x1, x2, s1, s2)\n",
    "        results.append((x1, x2))\n",
    "    print(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}')\n",
    "    return results\n",
    "\n",
    "def show_trace_2d(f, results): \n",
    "    \"\"\"Show the trace of 2D variables during optimization.\"\"\"\n",
    "    dl.set_figsize()\n",
    "    dl.plt.plot(*zip(*results), '-o', color='#ff7f0e')\n",
    "    x1, x2 = tf.meshgrid(tf.range(-5.5, 1.0, 0.1), tf.range(-3.0, 1.0, 0.1))\n",
    "    dl.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\n",
    "    dl.plt.xlabel('x1')\n",
    "    dl.plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 18,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def f_2d(x1, x2):  # Objective function\n",
    "    return x1**2 + 2 * x2**2\n",
    "\n",
    "def f_2d_grad(x1, x2):  # Gradient of the objective function\n",
    "    return (2 * x1, 4 * x2)\n",
    "\n",
    "def gd_2d(x1, x2, s1, s2, f_grad):\n",
    "    g1, g2 = f_grad(x1, x2)\n",
    "    return (x1 - eta * g1, x2 - eta * g2, 0, 0)\n",
    "\n",
    "eta = 0.1\n",
    "show_trace_2d(f_2d, train_2d(gd_2d, f_grad=f_2d_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "## Adaptive Methods\n",
    "\n",
    "\n",
    "### Newton's Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 20,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "c = tf.constant(0.5)\n",
    "\n",
    "def f(x):  # Objective function\n",
    "    return tf.cosh(c * x)\n",
    "\n",
    "def f_grad(x):  # Gradient of the objective function\n",
    "    return c * tf.sinh(c * x)\n",
    "\n",
    "def f_hess(x):  # Hessian of the objective function\n",
    "    return c**2 * tf.cosh(c * x)\n",
    "\n",
    "def newton(eta=1):\n",
    "    x = 10.0\n",
    "    results = [x]\n",
    "    for i in range(10):\n",
    "        x -= eta * f_grad(x) / f_hess(x)\n",
    "        results.append(float(x))\n",
    "    print('epoch 10, x:', x)\n",
    "    return results\n",
    "\n",
    "show_trace(newton(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "c = tf.constant(0.15 * np.pi)\n",
    "\n",
    "def f(x):  # Objective function\n",
    "    return x * tf.cos(c * x)\n",
    "\n",
    "def f_grad(x):  # Gradient of the objective function\n",
    "    return tf.cos(c * x) - c * x * tf.sin(c * x)\n",
    "\n",
    "def f_hess(x):  # Hessian of the objective function\n",
    "    return -2 * c * tf.sin(c * x) - x * c**2 * tf.cos(c * x)\n",
    "\n",
    "show_trace(newton(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 24,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "show_trace(newton(0.5), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Experiment with different learning rates and objective functions for gradient descent.\n",
    "1. Implement line search to minimize a convex function in the interval $[a, b]$.\n",
    "    1. Do you need derivatives for binary search, i.e., to decide whether to pick $[a, (a+b)/2]$ or $[(a+b)/2, b]$.\n",
    "    1. How rapid is the rate of convergence for the algorithm?\n",
    "    1. Implement the algorithm and apply it to minimizing $\\log (\\exp(x) + \\exp(-2x -3))$.\n",
    "1. Design an objective function defined on $\\mathbb{R}^2$ where gradient descent is exceedingly slow. Hint: scale different coordinates differently.\n",
    "1. Implement the lightweight version of Newton's method using preconditioning:\n",
    "    1. Use diagonal Hessian as preconditioner.\n",
    "    1. Use the absolute values of that rather than the actual (possibly signed) values.\n",
    "    1. Apply this to the problem above.\n",
    "1. Apply the algorithm above to a number of objective functions (convex or not). What happens if you rotate coordinates by $45$ degrees?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
