{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Adadelta\n",
    "\n",
    "## The Algorithm\n",
    "\n",
    "Given the parameter du jour is $\\rho$, we obtain the following leaky updates similarly to `RMSProp`:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\mathbf{s}_t & = \\rho \\mathbf{s}_{t-1} + (1 - \\rho) \\mathbf{g}_t^2.\n",
    "\\end{aligned}$$\n",
    "\n",
    "The difference to RMSProp is that we perform updates with the rescaled gradient $\\mathbf{g}_t'$, i.e.,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\mathbf{x}_t  & = \\mathbf{x}_{t-1} - \\mathbf{g}_t'. \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "So what is the rescaled gradient $\\mathbf{g}_t'$?\n",
    "\n",
    "We can calculate it as follows: $$\\begin{aligned}\n",
    "    \\mathbf{g}_t' & = \\frac{\\sqrt{\\Delta\\mathbf{x}_{t-1} + \\epsilon}}{\\sqrt{{\\mathbf{s}_t + \\epsilon}}} \\odot \\mathbf{g}_t, \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $\\Delta \\mathbf{x}_{t-1}$ is the leaky average of the squared rescaled gradients $\\mathbf{g}_t'$. We initialize $\\Delta \\mathbf{x}_{0}$ to be $0$ and update it at each step with $\\mathbf{g}_t'$, i.e.,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\Delta \\mathbf{x}_t & = \\rho \\Delta\\mathbf{x}_{t-1} + (1 - \\rho) {\\mathbf{g}_t'}^2,\n",
    "\\end{aligned}$$\n",
    "\n",
    "and $\\epsilon$ (a small value such as $10^{-5}$) is added to maintain numerical stability.\n",
    "\n",
    "\n",
    "\n",
    "## Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from dl import tensorflow as dl\n",
    "\n",
    "\n",
    "def init_adadelta_states(feature_dim):\n",
    "    s_w = tf.Variable(tf.zeros((feature_dim, 1)))\n",
    "    s_b = tf.Variable(tf.zeros(1))\n",
    "    delta_w = tf.Variable(tf.zeros((feature_dim, 1)))\n",
    "    delta_b = tf.Variable(tf.zeros(1))\n",
    "    return ((s_w, delta_w), (s_b, delta_b))\n",
    "\n",
    "def adadelta(params, grads, states, hyperparams):\n",
    "    rho, eps = hyperparams['rho'], 1e-5\n",
    "    for p, (s, delta), grad in zip(params, states, grads):\n",
    "        s[:].assign(rho * s + (1 - rho) * tf.math.square(grad))\n",
    "        g = (tf.math.sqrt(delta + eps) / tf.math.sqrt(s + eps)) * grad\n",
    "        p[:].assign(p - g)\n",
    "        delta[:].assign(rho * delta + (1 - rho) * g * g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "Choosing $\\rho = 0.9$ amounts to a half-life time of 10 for each parameter update. This tends to work quite well. We get the following behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 5,
    "scrolled": true,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "data_iter, feature_dim = dl.get_data_ch11(batch_size=10)\n",
    "dl.train_ch11(adadelta, init_adadelta_states(feature_dim), {'rho': 0.9},\n",
    "               data_iter, feature_dim);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "# adadelta is not converging at default learning rate\n",
    "# but it's converging at lr = 5.0\n",
    "trainer = tf.keras.optimizers.Adadelta\n",
    "dl.train_concise_ch11(trainer, {'learning_rate': 5.0, 'rho': 0.9}, data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. Adjust the value of $\\rho$. What happens?\n",
    "1. Show how to implement the algorithm without the use of $\\mathbf{g}_t'$. Why might this be a good idea?\n",
    "1. Is Adadelta really learning rate free? Could you find optimization problems that break Adadelta?\n",
    "1. Compare Adadelta to Adagrad and RMS prop to discuss their convergence behavior.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "140.8000030517578px",
    "left": "654px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
