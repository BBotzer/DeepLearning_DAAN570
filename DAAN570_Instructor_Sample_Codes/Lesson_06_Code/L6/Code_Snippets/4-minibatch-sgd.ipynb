{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Minibatch Stochastic Gradient Descent\n",
    "\n",
    "## Vectorization and Caches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from dl import tensorflow as dl\n",
    "\n",
    "timer = dl.Timer()\n",
    "A = tf.Variable(tf.zeros((256, 256)))\n",
    "B = tf.Variable(tf.random.normal([256, 256], 0, 1))\n",
    "C = tf.Variable(tf.random.normal([256, 256], 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "origin_pos": 7,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "# Compute A = BC one element at a time\n",
    "timer.start()\n",
    "for i in range(256):\n",
    "    for j in range(256):\n",
    "        A[i, j].assign(tf.tensordot(B[i, :], C[:, j], axes=1))\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "timer.start()\n",
    "for j in range(256):\n",
    "    A[:, j].assign(tf.tensordot(B, C[:, j], axes=1))\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 15,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "timer.start()\n",
    "A.assign(tf.tensordot(B, C, axes=1))\n",
    "timer.stop()\n",
    "\n",
    "# Multiply and add count as separate operations (fused in practice)\n",
    "gigaflops = [2 / i for i in timer.times]\n",
    "print(f'performance in Gigaflops: element {gigaflops[0]:.3f}, '\n",
    "      f'column {gigaflops[1]:.3f}, full {gigaflops[2]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "## Minibatches\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 19,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "timer.start()\n",
    "for j in range(0, 256, 64):\n",
    "    A[:, j:j + 64].assign(tf.tensordot(B, C[:, j:j + 64], axes=1))\n",
    "timer.stop()\n",
    "print(f'performance in Gigaflops: block {2 / timer.times[3]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "\n",
    "## Reading the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 23,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "dl.DATA_HUB['airfoil'] = (dl.DATA_URL + 'airfoil_self_noise.dat',\n",
    "                           '76e5be1548fd8222e5074cf0faae75edff8cf93f')\n",
    "\n",
    "def get_data_ch11(batch_size=10, n=1500):\n",
    "    data = np.genfromtxt(dl.download('airfoil'), dtype=np.float32,\n",
    "                         delimiter='\\t')\n",
    "    data = (data - data.mean(axis=0)) / data.std(axis=0)\n",
    "    data_iter = dl.load_array((data[:n, :-1], data[:n, -1]), batch_size,\n",
    "                               is_train=True)\n",
    "    return data_iter, data.shape[1] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "## Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 27,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def sgd(params, grads, states, hyperparams):\n",
    "    for param, grad in zip(params, grads):\n",
    "        param.assign_sub(hyperparams['lr'] * grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 31,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_ch11(trainer_fn, states, hyperparams, data_iter, feature_dim,\n",
    "               num_epochs=2):\n",
    "    # Initialization\n",
    "    w = tf.Variable(\n",
    "        tf.random.normal(shape=(feature_dim, 1), mean=0, stddev=0.01),\n",
    "        trainable=True)\n",
    "    b = tf.Variable(tf.zeros(1), trainable=True)\n",
    "\n",
    "    # Train\n",
    "    net, loss = lambda X: dl.linreg(X, w, b), dl.squared_loss\n",
    "    animator = dl.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n",
    "    n, timer = 0, dl.Timer()\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        for X, y in data_iter:\n",
    "            with tf.GradientTape() as g:\n",
    "                l = tf.math.reduce_mean(loss(net(X), y))\n",
    "\n",
    "            dw, db = g.gradient(l, [w, b])\n",
    "            trainer_fn([w, b], [dw, db], states, hyperparams)\n",
    "            n += X.shape[0]\n",
    "            if n % 200 == 0:\n",
    "                timer.stop()\n",
    "                p = n / X.shape[0]\n",
    "                q = p / tf.data.experimental.cardinality(data_iter).numpy()\n",
    "                r = (dl.evaluate_loss(net, data_iter, loss),)\n",
    "                animator.add(q, r)\n",
    "                timer.start()\n",
    "    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')\n",
    "    return timer.cumsum(), animator.Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 33,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def train_sgd(lr, batch_size, num_epochs=2):\n",
    "    data_iter, feature_dim = get_data_ch11(batch_size)\n",
    "    return train_ch11(sgd, None, {'lr': lr}, data_iter, feature_dim,\n",
    "                      num_epochs)\n",
    "\n",
    "gd_res = train_sgd(1, 1500, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 35,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "sgd_res = train_sgd(0.005, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 37,
    "scrolled": true,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "mini1_res = train_sgd(.4, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 39,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "mini2_res = train_sgd(.05, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 41,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "dl.set_figsize([6, 3])\n",
    "dl.plot(*list(map(list, zip(gd_res, sgd_res, mini1_res, mini2_res))),\n",
    "         'time (sec)', 'loss', xlim=[1e-2, 10],\n",
    "         legend=['gd', 'sgd', 'batch size=100', 'batch size=10'])\n",
    "dl.plt.gca().set_xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 42
   },
   "source": [
    "## Concise Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 45,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=2):\n",
    "    # Initialization\n",
    "    net = tf.keras.Sequential()\n",
    "    net.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            1, kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    optimizer = trainer_fn(**hyperparams)\n",
    "    loss = tf.keras.losses.MeanSquaredError()\n",
    "    # Note: L2 Loss = 1/2 * MSE Loss. TensorFlow has MSE Loss which is\n",
    "    # slightly different from MXNet's L2Loss by a factor of 2. Hence we halve\n",
    "    # the loss value to get L2Loss in TensorFlow\n",
    "    animator = dl.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n",
    "    n, timer = 0, dl.Timer()\n",
    "    for _ in range(num_epochs):\n",
    "        for X, y in data_iter:\n",
    "            with tf.GradientTape() as g:\n",
    "                out = net(X)\n",
    "                l = loss(y, out) / 2\n",
    "                params = net.trainable_variables\n",
    "                grads = g.gradient(l, params)\n",
    "            optimizer.apply_gradients(zip(grads, params))\n",
    "            n += X.shape[0]\n",
    "            if n % 200 == 0:\n",
    "                timer.stop()\n",
    "                p = n / X.shape[0]\n",
    "                q = p / tf.data.experimental.cardinality(data_iter).numpy()\n",
    "                r = (dl.evaluate_loss(net, data_iter, loss) / 2,)\n",
    "                animator.add(q, r)\n",
    "                timer.start()\n",
    "    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 49,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "data_iter, _ = get_data_ch11(10)\n",
    "trainer = tf.keras.optimizers.SGD\n",
    "train_concise_ch11(trainer, {'learning_rate': 0.05}, data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 50
   },
   "source": [
    "\n",
    "## Exercises\n",
    "\n",
    "1. Modify the batch size and learning rate and observe the rate of decline for the value of the objective function and the time consumed in each epoch.\n",
    "1. Compare minibatch stochastic gradient descent with a variant that actually *samples with replacement* from the training set. What happens?\n",
    "1. An evil genie replicates your dataset without telling you (i.e., each observation occurs twice and your dataset grows to twice its original size, but nobody told you). How does the behavior of stochastic gradient descent, minibatch stochastic gradient descent and that of gradient descent change?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
