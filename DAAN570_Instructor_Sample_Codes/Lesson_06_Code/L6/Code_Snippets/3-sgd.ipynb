{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from dl import tensorflow as dl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## Stochastic Gradient Updates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 5,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def f(x1, x2):  # Objective function\n",
    "    return x1**2 + 2 * x2**2\n",
    "\n",
    "def f_grad(x1, x2):  # Gradient of the objective function\n",
    "    return 2 * x1, 4 * x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def sgd(x1, x2, s1, s2, f_grad):\n",
    "    g1, g2 = f_grad(x1, x2)\n",
    "    # Simulate noisy gradient\n",
    "    g1 += tf.random.normal([1], 0.0, 1)\n",
    "    g2 += tf.random.normal([1], 0.0, 1)\n",
    "    eta_t = eta * lr()\n",
    "    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 8,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def constant_lr():\n",
    "    return 1\n",
    "\n",
    "eta = 0.1\n",
    "lr = constant_lr  # Constant learning rate\n",
    "dl.show_trace_2d(f, dl.train_2d(sgd, steps=50, f_grad=f_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "\n",
    "## Dynamic Learning Rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def exponential_lr():\n",
    "    # Global variable that is defined outside this function and updated inside\n",
    "    global t\n",
    "    t += 1\n",
    "    return math.exp(-0.1 * t)\n",
    "\n",
    "t = 1\n",
    "lr = exponential_lr\n",
    "dl.show_trace_2d(f, dl.train_2d(sgd, steps=1000, f_grad=f_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def polynomial_lr():\n",
    "    # Global variable that is defined outside this function and updated inside\n",
    "    global t\n",
    "    t += 1\n",
    "    return (1 + 0.1 * t)**(-0.5)\n",
    "\n",
    "t = 1\n",
    "lr = polynomial_lr\n",
    "dl.show_trace_2d(f, dl.train_2d(sgd, steps=50, f_grad=f_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. Experiment with different learning rate schedules for stochastic gradient descent and with different numbers of iterations. In particular, plot the distance from the optimal solution $(0, 0)$ as a function of the number of iterations.\n",
    "1. Prove that for the function $f(x_1, x_2) = x_1^2 + 2 x_2^2$ adding normal noise to the gradient is equivalent to minimizing a loss function $f(\\mathbf{x}, \\mathbf{w}) = (x_1 - w_1)^2 + 2 (x_2 - w_2)^2$ where $\\mathbf{x}$ is drawn from a normal distribution.\n",
    "1. Compare convergence of stochastic gradient descent when you sample from $\\{(x_1, y_1), \\ldots, (x_n, y_n)\\}$ with replacement and when you sample without replacement.\n",
    "1. How would you change the stochastic gradient descent solver if some gradient (or rather some coordinate associated with it) was consistently larger than all the other gradients?\n",
    "1. Assume that $f(x) = x^2 (1 + \\sin x)$. How many local minima does $f$ have? Can you change $f$ in such a way that to minimize it one needs to evaluate all the local minima?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
