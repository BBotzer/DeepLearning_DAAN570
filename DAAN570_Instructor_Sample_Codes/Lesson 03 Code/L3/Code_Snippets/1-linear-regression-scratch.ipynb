{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "tags": [
     "linear",
     "regression"
    ]
   },
   "source": [
    "# Linear Regression Implementation from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Linear-Regression-Implementation-from-Scratch\" data-toc-modified-id=\"Linear-Regression-Implementation-from-Scratch-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Linear Regression Implementation from Scratch</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generating-the-Dataset\" data-toc-modified-id=\"Generating-the-Dataset-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Generating the Dataset</a></span></li><li><span><a href=\"#Reading-the-Dataset\" data-toc-modified-id=\"Reading-the-Dataset-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Reading the Dataset</a></span></li><li><span><a href=\"#Initializing-Model-Parameters\" data-toc-modified-id=\"Initializing-Model-Parameters-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Initializing Model Parameters</a></span></li><li><span><a href=\"#Defining-the-Model\" data-toc-modified-id=\"Defining-the-Model-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Defining the Model</a></span></li><li><span><a href=\"#Defining-the-Loss-Function\" data-toc-modified-id=\"Defining-the-Loss-Function-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Defining the Loss Function</a></span></li><li><span><a href=\"#Defining-the-Optimization-Algorithm\" data-toc-modified-id=\"Defining-the-Optimization-Algorithm-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Defining the Optimization Algorithm</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Summary</a></span></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Exercises</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow \u001b[38;5;28;01mas\u001b[39;00m dl\n",
      "File \u001b[0;32m~/Data/Courses/Deep Learning/-DL- Neo/- Programming/DL/L2/Code_Snippets/dl/tensorflow.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from dl import tensorflow as dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## Generating the Dataset\n",
    "\n",
    "We generate a synthetic dataset,\n",
    "according to a linear model by adding noise and visualize it.\n",
    "\n",
    "The generated dataset contains 1000 examples, each consisting of 2 features\n",
    "sampled from a standard normal distribution. The dataset is a matrix\n",
    "$\\mathbf{X}\\in \\mathbb{R}^{1000 \\times 2}$.\n",
    "\n",
    "The true parameters that generate the synthetic dataset are:\n",
    "$\\mathbf{w} = [2, -3.4]^\\top$ and $b = 4.2$.\n",
    "\n",
    "The corresponding labels will be assigned according\n",
    "to the following linear model with the additive noise $\\epsilon$:\n",
    "\n",
    "$$\\mathbf{y}= \\mathbf{X} \\mathbf{w} + b + \\mathbf\\epsilon.$$\n",
    "\n",
    "You could think of $\\epsilon$ as capturing potential\n",
    "measurement errors on the features and labels.\n",
    "\n",
    "We will assume that the standard assumptions hold and thus\n",
    "that $\\epsilon$ obeys a normal distribution with mean of 0.\n",
    "To make our problem easy, we will set its standard deviation to 0.01.\n",
    "\n",
    "The following code generates our synthetic dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = tf.zeros((num_examples, w.shape[0]))\n",
    "    X += tf.random.normal(shape=X.shape)\n",
    "    y = tf.matmul(X, tf.reshape(w, (-1, 1))) + b\n",
    "    y += tf.random.normal(shape=y.shape, stddev=0.01)\n",
    "    y = tf.reshape(y, (-1, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "true_w = tf.constant([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "Each row in `features` consists of a 2-dimensional data example.\n",
    "\n",
    "Each row in `labels` consists of a 1-dimensional label value (a scalar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "print('features:', features[0], '\\nlabel:', labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "By using a scatter plot to visualize the second feature `features[:, 1]` and `labels`, we observe the linear correlation between the two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "dl.set_figsize()\n",
    "# The semicolon is for displaying the plot only\n",
    "dl.plt.scatter(features[:, (1)].numpy(), labels.numpy(), 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "## Reading the Dataset\n",
    "\n",
    "In Deep Learning and Machine Learning, training models require\n",
    "multiple passes over the dataset to progressively update the parameters. each pass consists of one minibatch of examples at a time.\n",
    "\n",
    "Since this process is a fundamental step\n",
    "to training machine learning algorithms, we define a utility function, called `data_iter` to shuffle the dataset and access it in minibatches.\n",
    "\n",
    "The `data_iter` function reads a batch size, a matrix of features,\n",
    "and a vector of labels, yielding minibatches of the size `batch_size`.\n",
    "\n",
    "Each minibatch consists of a tuple of features and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = tf.constant(indices[i:min(i + batch_size, num_examples)])\n",
    "        yield tf.gather(features, j), tf.gather(labels, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "\n",
    "Let us read and print the first small batch of data examples.\n",
    "\n",
    "The shape of the features in each minibatch shows the minibatch size and the number of input features.\n",
    "\n",
    "The minibatch of labels has a shape given by `batch_size`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 16,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, '\\n', y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "As we run the iteration, we obtain distinct minibatches\n",
    "successively until the entire dataset has been exhausted. For the sake of this example, we added the `break` to only print one batch instead of all generated minibatches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "**Note:**\n",
    "\n",
    "The iteration implemented in `data_iter` function is a good for didactic purposes but it is inefficient with large datasets. In fact, it loads all the data in memory to perform lots of random memory access. However, the built-in iterators implemented in deep learning frameworks are more efficient and they can deal\n",
    "with both data stored in files and data fed via data streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "## Initializing Model Parameters\n",
    "\n",
    "The next step is to initialize the model paramters \n",
    "\n",
    "Before training our model to optimize its parameters (weights and biases) using minibatch stochastic gradient descent, we have to initialize them.\n",
    "\n",
    "One possible way to initialize parameters is by sampling\n",
    "random numbers from a normal distribution with mean 0\n",
    "and a standard deviation of 0.01, and setting the bias to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 20,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random.normal(shape=(2, 1), mean=0, stddev=0.01),\n",
    "                trainable=True)\n",
    "b = tf.Variable(tf.zeros(1), trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "After initializing model's parameters,\n",
    "the next task is to update them by computing the gradient\n",
    "of our loss function. Given this gradient at each minibatch, we update parameters in the direction that may reduce the loss them until they fit our data well.\n",
    "\n",
    "Computing the gradients explicitly is a difficult task and error prone,\n",
    "we use automatic differentiation with the `autograd` function to compute the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "## Defining the Model\n",
    "\n",
    "By defining our model we associate inputs and parameters to its outputs.\n",
    "Since we are computing the output of a linear model, we simply take the matrix-vector dot product of the input features $\\mathbf{X}$ and the model weights $\\mathbf{w}$, and then add the offset $b$ to each example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "\n",
    "In the following code the $\\mathbf{Xw}$  is a vector and $b$ is a scalar.\n",
    "\n",
    "In addition, the broadcasting mechanism is explicitly applied when adding a vector and a scalar. By such, the scalar is added to each component of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def linreg(X, w, b):  #@save\n",
    "    \"\"\"The linear regression model.\"\"\"\n",
    "    return tf.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 23
   },
   "source": [
    "## Defining the Loss Function\n",
    "\n",
    "Since we need to update our model by computing the gradient of our loss function, we decide to define the loss function as the squared root.\n",
    "\n",
    "\n",
    "The squared loss function described in the `linear_regression` lesson to compute the difference between the true value `y` and the predicted value. Note that we need to shape `y` as `y_hat` before computing the difference.\n",
    "\n",
    "The result returned by the following function\n",
    "will also have the same shape as $y_hat$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 24,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"Squared loss.\"\"\"\n",
    "    return (y_hat - tf.reshape(y, y_hat.shape))**2 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "## Defining the Optimization Algorithm\n",
    "\n",
    "As we discussed in the lesson, the `linear_regression` has a closed-form solution and can be solved analytically. \n",
    "However, none of the other machine learning models that we study in this course\n",
    "can be solved analytically, we will take this opportunity to introduce your first working example of minibatch stochastic gradient descent. This solution works for all models.\n",
    "\n",
    "\n",
    "\n",
    "At each step:\n",
    "- we use one minibatch randomly drawn from our dataset,\n",
    "- we estimate the gradient of the loss with respect to our parameters.\n",
    "- we  update our parameters in the direction that may reduce the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 28,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def sgd(params, grads, lr, batch_size):  #@save\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "    for param, grad in zip(params, grads):\n",
    "        param.assign_sub(lr * grad / batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "Note that this code code applies the minibatch stochastic gradient descent update,\n",
    "given a set of parameters, a learning rate, and a batch size.\n",
    "The size of the update step is determined by the learning rate `lr`.\n",
    "\n",
    "Because our loss is calculated as a sum over the minibatch of examples,\n",
    "we normalize our step size by the batch size (`batch_size`),\n",
    "so that the magnitude of a typical step size\n",
    "does not depend heavily on our choice of the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 29
   },
   "source": [
    "## Training\n",
    "\n",
    "To train our model, we implement the main training loop.\n",
    "\n",
    "It is important that you understand this snippet of code. \n",
    "You will see nearly identical training loops\n",
    "over and over again in all your deep learning projects.\n",
    "\n",
    "1. In each iteration, we read a minibatch of training examples,\n",
    "and pass them through our model to obtain a set of predictions.\n",
    "\n",
    "2. After apply the loss function to calculate the loss (total error), we initiate the backwards pass through the network, storing the gradients with respect to each parameter.\n",
    "\n",
    "3. Finally, we invoke the optimization algorithm `sgd` to update the model parameters.\n",
    "\n",
    "In summary, we will execute the following loop:\n",
    "\n",
    "* Initialize parameters $(\\mathbf{w}, b)$\n",
    "* Repeat until done\n",
    "    * Compute gradient $\\mathbf{g} \\leftarrow \\partial_{(\\mathbf{w},b)} \\frac{1}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} l(\\mathbf{x}^{(i)}, y^{(i)}, \\mathbf{w}, b)$\n",
    "    * Update parameters $(\\mathbf{w}, b) \\leftarrow (\\mathbf{w}, b) - \\eta \\mathbf{g}$\n",
    "\n",
    "In each *epoch*,\n",
    "we  iterate through the entire dataset (using the `data_iter` function) once\n",
    "passing through every example in the training dataset\n",
    "(assuming that the number of examples is divisible by the batch size).\n",
    "\n",
    "**Hyperparameters:**\n",
    "- The number of epochs `num_epochs` and the learning rate `lr` are both hyperparameters, which we set here to 3 and 0.03, respectively. Try to change their values and rerun the training function. what is your observation? is it possible to improve the model performance? \n",
    "\n",
    "- You may conclude that the setting of hyperparameters is tricky\n",
    "and requires tuning some adjustment by trial and error.\n",
    "We will explain the these details for now but revise them\n",
    "later in the optimization lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 3\n",
    "net = linreg\n",
    "loss = squared_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 33,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        with tf.GradientTape() as g:\n",
    "            l = loss(net(X, w, b), y)  # Minibatch loss in `X` and `y`\n",
    "        # Compute gradient on l with respect to [`w`, `b`]\n",
    "        dw, db = g.gradient(l, [w, b])\n",
    "        # Update parameters using their gradient\n",
    "        sgd([w, b], [dw, db], lr, batch_size)\n",
    "    train_l = loss(net(features, w, b), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {float(tf.reduce_mean(train_l)):f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "In this case-study, we synthesized the dataset and we thus know in advanced what the true parameters are.\n",
    "\n",
    "Consequently , we can easily evaluate our success in training\n",
    "by comparing the true parameters\n",
    "with parameters that are learned through our training loop.\n",
    "\n",
    "\n",
    "It turns out that they are very close to each other!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 35,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "print(f'error in estimating w: {true_w - tf.reshape(w, true_w.shape)}')\n",
    "print(f'error in estimating b: {true_b - b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 36
   },
   "source": [
    "Note that we should not take it for granted\n",
    "that we are able to recover the parameters perfectly.\n",
    "\n",
    "\n",
    "In machine learning, we are typically less concerned\n",
    "with recovering true underlying parameters,\n",
    "and more concerned with parameters that make our model generalize to unforeseen  examples.\n",
    "\n",
    "Even on difficult optimization problems, the stochastic gradient descent often finds good solutions. Note that for deep networks, many configurations of the parameters are possible and may lead to highly accurate prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 36
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Deep networks can be implemented and optimized from scratch, using just tensors and auto differentiation, without any need for defining layers or fancy optimizers.\n",
    "\n",
    "* This case-study only scratches the surface of what is possible with deep neural networks. In the following sections, we will describe additional models based on the concepts that we have just introduced and implement them more concisely with high-level TensorFlow API/libraries.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 36
   },
   "source": [
    "\n",
    "## Exercises\n",
    "\n",
    "1. What would happen if we were to initialize the weights to zero. Would the algorithm still work?\n",
    "1. Assume that you are\n",
    "   [Georg Simon Ohm](https://en.wikipedia.org/wiki/Georg_Ohm) trying to come up\n",
    "   with a model between voltage and current. Can you use auto differentiation to learn the parameters of your model?\n",
    "1. Can you use [Planck's Law](https://en.wikipedia.org/wiki/Planck%27s_law) to determine the temperature of an object using spectral energy density?\n",
    "1. What are the problems you might encounter if you wanted to  compute the second derivatives? How would you fix them?\n",
    "1.  Why is the `reshape` function needed in the `squared_loss` function?\n",
    "1. Experiment using different learning rates to find out how fast the loss function value drops.\n",
    "1. If the number of examples cannot be divided by the batch size, what happens to the `data_iter` function's behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0,
    "tags": [
     "linear",
     "regression"
    ]
   },
   "source": [
    "In this notebook, we implement the entire linear regression from scratch. The implementation covers the data preprocessing, the model building, the loss function, and the minibatch stochastic gradient descent optimizer.\n",
    "\n",
    "We only use tensors and auto differentiation to implement the regression from scratch \n",
    "to make sure that you understand how it works.\n",
    "\n",
    "We will provide an alternative and more concise implementation using the TensorFlow Keras library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
