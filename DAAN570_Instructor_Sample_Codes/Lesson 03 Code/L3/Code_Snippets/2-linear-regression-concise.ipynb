{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Implementation of Linear Regression with TensorFlow\n",
    "\n",
    "TensorFlow is an industry- grade open-source framework for automating the repetitive work of implementing gradient-based learning algorithms.\n",
    "\n",
    "In the previous implementation from scratch, we relied only on:\n",
    "1. Tensors for data storage and linear algebra;\n",
    "2. Auto differentiation for calculating gradients.\n",
    "\n",
    "In practice, because data iterators, loss functions, optimizers,\n",
    "and neural network layers\n",
    "are so common, TensorFlow libraries implement and optimize these functions for us!\n",
    "\n",
    "In this section, you learn how to implement the linear regression model concisely by using the `keras` high-level APIs in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Implementation-of-Linear-Regression-with-TensorFlow\" data-toc-modified-id=\"Implementation-of-Linear-Regression-with-TensorFlow-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Implementation of Linear Regression with TensorFlow</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generating-the-Dataset\" data-toc-modified-id=\"Generating-the-Dataset-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Generating the Dataset</a></span></li><li><span><a href=\"#Reading-the-Dataset\" data-toc-modified-id=\"Reading-the-Dataset-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Reading the Dataset</a></span></li><li><span><a href=\"#Defining-the-Model\" data-toc-modified-id=\"Defining-the-Model-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Defining the Model</a></span></li><li><span><a href=\"#Initializing-Model-Parameters\" data-toc-modified-id=\"Initializing-Model-Parameters-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Initializing Model Parameters</a></span></li><li><span><a href=\"#Defining-the-Loss-Function\" data-toc-modified-id=\"Defining-the-Loss-Function-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Defining the Loss Function</a></span></li><li><span><a href=\"#Defining-the-Optimization-Algorithm\" data-toc-modified-id=\"Defining-the-Optimization-Algorithm-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Defining the Optimization Algorithm</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Summary</a></span></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Exercises</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "## Generating the Dataset\n",
    "\n",
    "Let us generate a synthetic dataset, according to a linear model in a similar way to the previous case-study.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from dl import tensorflow as dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 4,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "true_w = tf.constant([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = dl.synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "## Reading the Dataset\n",
    "\n",
    "Instead of invoking our own iterator (`data_iter` function),\n",
    "we invoke the existing TensorFlow API to read data from tensor and construct a TensorFlow data iterator.\n",
    "\n",
    "\n",
    "The `load_array` function takes `features` and `labels` as arguments, specifies `batch_size` and  instantes a data iterator object.\n",
    "\n",
    "Note that the boolean value `is_train` indicates whether or not\n",
    "the data iterator object has to shuffle the data on each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 8,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"Construct a TensorFlow data iterator.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data_arrays)\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "Now we use `data_iter` variable in much the same way as we called\n",
    "the `data_iter` function in the previous case-study. \n",
    "\n",
    "To verify that the `data_iter` is working properly, we read and print\n",
    "the first minibatch of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "In this code, we use `iter` to construct a Python iterator and use `next` to obtain the first item from the iterator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "## Defining the Model\n",
    "\n",
    "In our implementation of linear regression from scratch\n",
    "we defined our model parameters explicitly\n",
    "and coded up the calculations to produce output\n",
    "using basic linear algebra operations.\n",
    "\n",
    "\n",
    "It is important that you understand and become familiar with this implementation. Doing it once or twice is rewarding and instructive.\n",
    "\n",
    "However, in real-world problems your models get more complex. For such standard operations, we use TensorFlow's predefined layers,\n",
    "which allow us to focus on the layers instead of low level details.\n",
    "\n",
    "To this end, we use the Keras API in TensorFlow, which provide nencessary modules such as `net`, `Sequential`, `MeanSquaredError` etc.\n",
    "\n",
    "Using Keras, we define a model variable `net`,\n",
    "which will refer to an instance of the `Sequential` class. The `Sequential` class chains several layers where the output of one layer is the input of the next layer and so forth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 15,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "Note that the layer is said to be *fully-connected*\n",
    "because each of its inputs is connected to each of its outputs\n",
    "by means of a matrix-vector multiplication.\n",
    "\n",
    "In Keras, the `Dense` class allows us to specify a fully-connected layer is. Since we only want to generate a single scalar output, we set that number to 1.\n",
    "\n",
    "For convenience, Keras does not require us to specify the input shape for each layer. For this reason, we do not specify how many inputs go into this linear layer. In fact, when we first pass data through our model by executing the `net(X)` function, Keras will automatically infer the number of inputs to each layer.\n",
    "\n",
    "In the following, we describe with details how to build the model concisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 18,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "# `keras` is the high-level API for TensorFlow\n",
    "net = tf.keras.Sequential()\n",
    "net.add(tf.keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "## Initializing Model Parameters\n",
    "\n",
    "To initialize the model parameters, namely the weights and bias in the linear regression model, we specify that each weight parameter\n",
    "should be randomly sampled from a normal distribution with mean 0 and standard deviation 0.01. In addition, we initialize the bias parameter with zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "Deep learning frameworks often have a predefined function to initialize parameters the model parameters, namely the weights and bias. TensorFlow has the `initializers` module which includes various methods for model parameter initialization. \n",
    "\n",
    "In Keras,the easiest way to specify the initialization method is to create the layer by specifying `kernel_initializer`. \n",
    "\n",
    "In the following snippet code we recreate the `net` function with Keras initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 25,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "initializer = tf.initializers.RandomNormal(stddev=0.01)\n",
    "net = tf.keras.Sequential()\n",
    "net.add(tf.keras.layers.Dense(1, kernel_initializer=initializer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 28,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "If you look closely to the code above you you should note\n",
    "that the parameters initialization is performed even though Keras does not yet know how many dimensions the input will have!\n",
    "\n",
    "You might ask whether it should be 2 as in our example or it might be 2000.\n",
    "In fact, Keras lets us get away with the dimensions because behind the scenes, the initialization is actually *deferred* and will only take place \n",
    "when we first time pass data through the network.\n",
    "\n",
    "Since the parameters have not been initialized yet you cannot access or manipulate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 29
   },
   "source": [
    "## Defining the Loss Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 32,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "In Keras, the `MeanSquaredError` class computes the mean squared error (a.k. squared $L_2$ norm) and returns the average loss over examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 35,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 36
   },
   "source": [
    "## Defining the Optimization Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 39,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "In Keras, the `optimizers` module implements the Minibatch stochastic gradient descent and many of its variations for optimizing neural networks.\n",
    "\n",
    "\n",
    "The Minibatch stochastic gradient descent takes the argument, `learning_rate`, which we set it to the value of 0.03.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 42,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "trainer = tf.keras.optimizers.SGD(learning_rate=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 43
   },
   "source": [
    "## Training\n",
    "\n",
    "Building neural networks with high-level APIs of a deep learning framework\n",
    "requires few lines of code. Many details are abstracted by the framework modules to allocate parameters,define loss functions, and implement minibatch stochastic gradient descent.\n",
    "\n",
    "\n",
    "All basic components are now defined and initialized. The training loop itself is similar to what we did when implementing the regression from scratch.\n",
    "\n",
    "For every epoch, we pass over the dataset (`train_data`) by iteratively grabbing one minibatch of inputs at a time and its corresponding ground-truth labels to compute the gradient.\n",
    "\n",
    "\n",
    "For each minibatch, we perform the following step:\n",
    "* Forward propagation\n",
    "    * Generate predictions by calling `net(X)` \n",
    "    * Calculate the loss `l`().\n",
    "\n",
    "* Backpropagation\n",
    "    * Calculate gradients by running the backpropagation.\n",
    "    * Update the model parameters by invoking our optimizer.\n",
    "\n",
    "In the following code, we compute and print the loss after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 46,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        with tf.GradientTape() as tape:\n",
    "            l = loss(net(X, training=True), y)\n",
    "        grads = tape.gradient(l, net.trainable_variables)\n",
    "        trainer.apply_gradients(zip(grads, net.trainable_variables))\n",
    "    l = loss(net(features), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 47
   },
   "source": [
    "The following code compares the model parameters learned by training on finite data and the actual parameters that synthetically generated our dataset.\n",
    "\n",
    "To to do so, we have to access the model's parameters. The first step is to access the layer that we need from `net` and then access that layer's weights and bias.\n",
    "\n",
    "Unsurprisingly,our estimated parameters are close to their ground-truth counterparts. We obtained similar results as in the from-scratch implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 50,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "w = net.get_weights()[0]\n",
    "print('error in estimating w', true_w - tf.reshape(w, true_w.shape))\n",
    "b = net.get_weights()[1]\n",
    "print('error in estimating b', true_b - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 51
   },
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 54,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "* TensorFlow's high-level APIs (Keras) allows us to implement models quickly.\n",
    "* The `data` module in TensorFlow provides tools for data processing.\n",
    "* The `keras` module provides a large number of neural network layers and common loss functions.\n",
    "* The TensorFlow's module `initializers` provides various methods for model parameter initialization.\n",
    "* Dimensionality and storage are automatically inferred.\n",
    "* Parameters cannot be accessed before they have been initialized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 55
   },
   "source": [
    "## Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 58,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "1. Review the TensorFlow documentation to see what loss functions and initialization methods are provided.\n",
    "2. Replace the loss by Huber's loss and rerun the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
