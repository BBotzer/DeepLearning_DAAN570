{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d1912f",
   "metadata": {},
   "source": [
    "# Example Taken from D2L: https://d2l.ai/chapter_multilayer-perceptrons/mlp-implementation.html#exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da0b95",
   "metadata": {},
   "source": [
    "Import the Required Library\n",
    "\n",
    "Let’s first import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9866a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'd2l'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01md2l\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow \u001b[38;5;28;01mas\u001b[39;00m d2l\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#@d2l.add_to_class(MLPScratch)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnet\u001b[39m(X): \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'd2l'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from d2l import tensorflow as d2l\n",
    "\n",
    "#@d2l.add_to_class(MLPScratch)\n",
    "def net(X): \n",
    "    X = tf.reshape(X, (-1, num_inputs)) \n",
    "    H = relu(tf.matmul(X, W1) + b1) \n",
    "    return tf.matmul(H, W2) + b2\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = dll.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48449eff",
   "metadata": {},
   "source": [
    "Initializing Model Parameters\n",
    "\n",
    "The Fashion-MNIST images contain small resolution images. Each image has a width of 28 pixels and a height of 28 pixels and belong to one of 10 possible classes for example, shirt, trouser, pullover etc.\n",
    "\n",
    "In order to feed the MLP, an image can be represented as vector of a 28×28=784\n",
    "\n",
    "Every pixel is a feature with values ranging from 0 to 255.\n",
    "\n",
    "The number of neurons in the hidden layer is a hyper parameter. A common practice is to choose the layer widths in powers of 2 to improve computation efficiency. In this implementation, we implement the one-hidden layer with 256 hidden units and we randomly initialize the weights matrices, W1 and W2 and the bias vectors, b1 and b2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35778d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "W1 = tf.Variable(\n",
    "  tf.random.normal(shape=(num_inputs, num_hiddens), mean=0, stddev=0.01))\n",
    "b1 = tf.Variable(tf.zeros(num_hiddens))\n",
    "W2 = tf.Variable(\n",
    "  tf.random.normal(shape=(num_hiddens, num_outputs), mean=0, stddev=0.01))\n",
    "b2 = tf.Variable(tf.random.normal([num_outputs], stddev=.01))\n",
    "\n",
    "params = [W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30a1ab6",
   "metadata": {},
   "source": [
    "Defining the Activation Function\n",
    "\n",
    "The next step consists of defining the activation function. We choose to implement the ReLU activation using the maximum function from the math library. Another option is to use the built-in RELU function directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f7c6b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "   return tf.math.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d1dbba",
   "metadata": {},
   "source": [
    "Defining the MLP Model\n",
    "\n",
    "As before mentioned, each training example is a two-dimensional image and you have to reshape it into flat vector of length num_inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "034f8f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "  X = tf.reshape(X, (-1, num_inputs))\n",
    "  H = relu(tf.matmul(X, W1) + b1)\n",
    "  return tf.matmul(H, W2) + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2687a3",
   "metadata": {},
   "source": [
    "Defining the Loss Function\n",
    "\n",
    "For our classification problem we choose the softmax activation function to implement the output neuron and the cross-entropy as the loss function to measure the error between the outcome y^\n",
    "and the true label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a3cf1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_hat, y):\n",
    "   return tf.losses.sparse_categorical_crossentropy(y, y_hat,\n",
    "           from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a979a",
   "metadata": {},
   "source": [
    "Training\n",
    "\n",
    "The training loop for MLPs is similar to the softmax regression model trained in Lesson 3. We choose the learning rate of   0.1 and the number of epochs of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5a090e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Updater'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epochs, lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m----> 2\u001b[0m updater \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUpdater\u001b[49m([W1, W2, b1, b2], lr)\n\u001b[0;32m      3\u001b[0m tf\u001b[38;5;241m.\u001b[39mtrain_ch3(net, train_iter, test_iter, loss, num_epochs, updater)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Updater'"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 10, 0.1\n",
    "updater = dl.Updater([W1, W2, b1, b2], lr)\n",
    "tf.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4321d13f",
   "metadata": {},
   "source": [
    " Prediction\n",
    "\n",
    "Once the MLP is trained, you can evaluate its performance on a small test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19721a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.predict_ch3(net, test_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
