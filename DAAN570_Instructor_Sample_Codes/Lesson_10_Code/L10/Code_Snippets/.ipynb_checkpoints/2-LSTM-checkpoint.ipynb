{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Long Short-Term Memory (LSTM)\n",
    "\n",
    "\n",
    "## Implementation from Scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from dl import tensorflow as dl\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = dl.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "### Initializing Model Parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def get_lstm_params(vocab_size, num_hiddens):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return tf.Variable(\n",
    "            tf.random.normal(shape=shape, stddev=0.01, mean=0,\n",
    "                             dtype=tf.float32))\n",
    "\n",
    "    def three():\n",
    "        return (normal(\n",
    "            (num_inputs, num_hiddens)), normal((num_hiddens, num_hiddens)),\n",
    "                tf.Variable(tf.zeros(num_hiddens), dtype=tf.float32))\n",
    "\n",
    "    W_xi, W_hi, b_i = three()  # Input gate parameters\n",
    "    W_xf, W_hf, b_f = three()  # Forget gate parameters\n",
    "    W_xo, W_ho, b_o = three()  # Output gate parameters\n",
    "    W_xc, W_hc, b_c = three()  # Candidate memory cell parameters\n",
    "    # Output layer parameters\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = tf.Variable(tf.zeros(num_outputs), dtype=tf.float32)\n",
    "    # Attach gradients\n",
    "    params = [\n",
    "        W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,\n",
    "        W_hq, b_q]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "### Defining the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def init_lstm_state(batch_size, num_hiddens):\n",
    "    return (tf.zeros(shape=(batch_size, num_hiddens)),\n",
    "            tf.zeros(shape=(batch_size, num_hiddens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 15,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def lstm(inputs, state, params):\n",
    "    W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q = params\n",
    "    (H, C) = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        X = tf.reshape(X, [-1, W_xi.shape[0]])\n",
    "        I = tf.sigmoid(tf.matmul(X, W_xi) + tf.matmul(H, W_hi) + b_i)\n",
    "        F = tf.sigmoid(tf.matmul(X, W_xf) + tf.matmul(H, W_hf) + b_f)\n",
    "        O = tf.sigmoid(tf.matmul(X, W_xo) + tf.matmul(H, W_ho) + b_o)\n",
    "        C_tilda = tf.tanh(tf.matmul(X, W_xc) + tf.matmul(H, W_hc) + b_c)\n",
    "        C = F * C + I * C_tilda\n",
    "        H = O * tf.tanh(C)\n",
    "        Y = tf.matmul(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return tf.concat(outputs, axis=0), (H, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "### Training and Prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 18,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "vocab_size, num_hiddens, device_name = len(\n",
    "    vocab), 256, dl.try_gpu()._device_name\n",
    "num_epochs, lr = 500, 1\n",
    "strategy = tf.distribute.OneDeviceStrategy(device_name)\n",
    "\n",
    "with strategy.scope():\n",
    "    model = dl.RNNModelScratch(len(vocab), num_hiddens, init_lstm_state,\n",
    "                                lstm, get_lstm_params)\n",
    "dl.train_dl(model, train_iter, vocab, lr, num_epochs, strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "## Concise Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "lstm_cell = tf.keras.layers.LSTMCell(num_hiddens,\n",
    "                                     kernel_initializer='glorot_uniform')\n",
    "lstm_layer = tf.keras.layers.RNN(lstm_cell, time_major=True,\n",
    "                                 return_sequences=True, return_state=True)\n",
    "device_name = dl.try_gpu()._device_name\n",
    "strategy = tf.distribute.OneDeviceStrategy(device_name)\n",
    "with strategy.scope():\n",
    "    model = dl.RNNModel(lstm_layer, vocab_size=len(vocab))\n",
    "dl.train_dl(model, train_iter, vocab, lr, num_epochs, strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 23
   },
   "source": [
    "\n",
    "## Exercises (Optional)\n",
    "\n",
    "1. Adjust the hyperparameters and analyze the their influence on running time, perplexity, and the output sequence.\n",
    "1. How would you need to change the model to generate proper words as opposed to sequences of characters?\n",
    "1. Compare the computational cost for GRUs, LSTMs, and regular RNNs for a given hidden dimension. Pay special attention to the training and inference cost.\n",
    "1. Since the candidate memory cell ensures that the value range is between $-1$ and $1$ by  using the $\\tanh$ function, why does the hidden state need to use the $\\tanh$ function again to ensure that the output value range is between $-1$ and $1$?\n",
    "1. Implement an LSTM model for time series prediction rather than character sequence prediction.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
