{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Recurrent Neural Networks\n","\n","## Recurrent Neural Networks with Hidden States"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%matplotlib inline\n","import math\n","\n","import tensorflow as tf\n","from dl import tensorflow as dl"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X, W_xh = tf.random.normal((3, 1), 0, 1), tf.random.normal((1, 4), 0, 1)\n","H, W_hh = tf.random.normal((3, 4), 0, 1), tf.random.normal((4, 4), 0, 1)\n","tf.matmul(X, W_xh) + tf.matmul(H, W_hh)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf.matmul(tf.concat((X, H), 1), tf.concat((W_xh, W_hh), 0))"]},{"cell_type":"markdown","metadata":{"origin_pos":0},"source":["# Implementation of Recurrent Neural Networks from Scratch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":4,"tab":["tensorflow"]},"outputs":[],"source":["batch_size, num_steps = 32, 35\n","train_iter, vocab = dl.load_data_time_machine(batch_size, num_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":5,"tab":["tensorflow"]},"outputs":[],"source":["train_random_iter, vocab_random_iter = dl.load_data_time_machine(\n","    batch_size, num_steps, use_random_iter=True)"]},{"cell_type":"markdown","metadata":{"origin_pos":6},"source":["## One-Hot Encoding\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":9,"tab":["tensorflow"]},"outputs":[],"source":["tf.one_hot(tf.constant([0, 2]), len(vocab))"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":13,"tab":["tensorflow"]},"outputs":[],"source":["X = tf.reshape(tf.range(10), (2, 5))\n","tf.one_hot(tf.transpose(X), 28).shape"]},{"cell_type":"markdown","metadata":{"origin_pos":14},"source":["## Initializing the Model Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":17,"tab":["tensorflow"]},"outputs":[],"source":["def get_params(vocab_size, num_hiddens):\n","    num_inputs = num_outputs = vocab_size\n","\n","    def normal(shape):\n","        return tf.random.normal(shape=shape, stddev=0.01, mean=0,\n","                                dtype=tf.float32)\n","\n","    # Hidden layer parameters\n","    W_xh = tf.Variable(normal((num_inputs, num_hiddens)), dtype=tf.float32)\n","    W_hh = tf.Variable(normal((num_hiddens, num_hiddens)), dtype=tf.float32)\n","    b_h = tf.Variable(tf.zeros(num_hiddens), dtype=tf.float32)\n","    # Output layer parameters\n","    W_hq = tf.Variable(normal((num_hiddens, num_outputs)), dtype=tf.float32)\n","    b_q = tf.Variable(tf.zeros(num_outputs), dtype=tf.float32)\n","    params = [W_xh, W_hh, b_h, W_hq, b_q]\n","    return params"]},{"cell_type":"markdown","metadata":{"origin_pos":18},"source":["## RNN Model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":21,"tab":["tensorflow"]},"outputs":[],"source":["def init_rnn_state(batch_size, num_hiddens):\n","    return (tf.zeros((batch_size, num_hiddens)),)"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":25,"tab":["tensorflow"]},"outputs":[],"source":["def rnn(inputs, state, params):\n","    # Here `inputs` shape: (`num_steps`, `batch_size`, `vocab_size`)\n","    W_xh, W_hh, b_h, W_hq, b_q = params\n","    H, = state\n","    outputs = []\n","    # Shape of `X`: (`batch_size`, `vocab_size`)\n","    for X in inputs:\n","        X = tf.reshape(X, [-1, W_xh.shape[0]])\n","        H = tf.tanh(tf.matmul(X, W_xh) + tf.matmul(H, W_hh) + b_h)\n","        Y = tf.matmul(H, W_hq) + b_q\n","        outputs.append(Y)\n","    return tf.concat(outputs, axis=0), (H,)"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":29,"tab":["tensorflow"]},"outputs":[],"source":["class RNNModelScratch:  \n","    \"\"\"A RNN Model implemented from scratch.\"\"\"\n","    def __init__(self, vocab_size, num_hiddens, init_state, forward_fn,\n","                 get_params):\n","        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n","        self.init_state, self.forward_fn = init_state, forward_fn\n","        self.trainable_variables = get_params(vocab_size, num_hiddens)\n","\n","    def __call__(self, X, state):\n","        X = tf.one_hot(tf.transpose(X), self.vocab_size)\n","        X = tf.cast(X, tf.float32)\n","        return self.forward_fn(X, state, self.trainable_variables)\n","\n","    def begin_state(self, batch_size, *args, **kwargs):\n","        return self.init_state(batch_size, self.num_hiddens)"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":33,"tab":["tensorflow"]},"outputs":[],"source":["# defining tensorflow training strategy\n","device_name = dl.try_gpu()._device_name\n","strategy = tf.distribute.OneDeviceStrategy(device_name)\n","\n","num_hiddens = 512\n","with strategy.scope():\n","    net = RNNModelScratch(len(vocab), num_hiddens, init_rnn_state, rnn,\n","                          get_params)\n","state = net.begin_state(X.shape[0])\n","Y, new_state = net(X, state)\n","Y.shape, len(new_state), new_state[0].shape"]},{"cell_type":"markdown","metadata":{"origin_pos":34},"source":["\n","## Prediction\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":37,"tab":["tensorflow"]},"outputs":[],"source":["def predict_ch8(prefix, num_preds, net, vocab):  \n","    \"\"\"Generate new characters following the `prefix`.\"\"\"\n","    state = net.begin_state(batch_size=1, dtype=tf.float32)\n","    outputs = [vocab[prefix[0]]]\n","    get_input = lambda: tf.reshape(tf.constant([outputs[-1]]), (1, 1)).numpy()\n","    for y in prefix[1:]:  # Warm-up period\n","        _, state = net(get_input(), state)\n","        outputs.append(vocab[y])\n","    for _ in range(num_preds):  # Predict `num_preds` steps\n","        y, state = net(get_input(), state)\n","        outputs.append(int(y.numpy().argmax(axis=1).reshape(1)))\n","    return ''.join([vocab.idx_to_token[i] for i in outputs])"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":40,"tab":["tensorflow"]},"outputs":[],"source":["predict_ch8('time traveller ', 10, net, vocab)"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":44,"tab":["tensorflow"]},"outputs":[],"source":["def grad_clipping(grads, theta): \n","    \"\"\"Clip the gradient.\"\"\"\n","    theta = tf.constant(theta, dtype=tf.float32)\n","    new_grad = []\n","    for grad in grads:\n","        if isinstance(grad, tf.IndexedSlices):\n","            new_grad.append(tf.convert_to_tensor(grad))\n","        else:\n","            new_grad.append(grad)\n","    norm = tf.math.sqrt(\n","        sum((tf.reduce_sum(grad**2)).numpy() for grad in new_grad))\n","    norm = tf.cast(norm, tf.float32)\n","    if tf.greater(norm, theta):\n","        for i, grad in enumerate(new_grad):\n","            new_grad[i] = grad * theta / norm\n","    else:\n","        new_grad = new_grad\n","    return new_grad"]},{"cell_type":"markdown","metadata":{"origin_pos":45},"source":["## Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":48,"tab":["tensorflow"]},"outputs":[],"source":["\n","def train_epoch_ch8(net, train_iter, loss, updater, use_random_iter):\n","    \"\"\"Train a model within one epoch\"\"\"\n","    state, timer = None, dl.Timer()\n","    metric = dl.Accumulator(2)  # Sum of training loss, no. of tokens\n","    for X, Y in train_iter:\n","        if state is None or use_random_iter:\n","            # Initialize `state` when either it is the first iteration or\n","            # using random sampling\n","            state = net.begin_state(batch_size=X.shape[0], dtype=tf.float32)\n","        with tf.GradientTape(persistent=True) as g:\n","            y_hat, state = net(X, state)\n","            y = tf.reshape(tf.transpose(Y), (-1))\n","            l = loss(y, y_hat)\n","        params = net.trainable_variables\n","        grads = g.gradient(l, params)\n","        grads = grad_clipping(grads, 1)\n","        updater.apply_gradients(zip(grads, params))\n","\n","        # Keras loss by default returns the average loss in a batch\n","        # l_sum = l * float(tf.size(y).numpy()) if isinstance(\n","        #     loss, tf.keras.losses.Loss) else tf.reduce_sum(l)\n","        metric.add(l * tf.size(y).numpy(), tf.size(y).numpy())\n","    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":52,"tab":["tensorflow"]},"outputs":[],"source":["\n","def train_ch8(net, train_iter, vocab, lr, num_epochs, strategy,\n","              use_random_iter=False):\n","    \"\"\"Train a model \"\"\"\n","    with strategy.scope():\n","        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","        updater = tf.keras.optimizers.SGD(lr)\n","    animator = dl.Animator(xlabel='epoch', ylabel='perplexity',\n","                            legend=['train'], xlim=[10, num_epochs])\n","    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab)\n","    # Train and predict\n","    for epoch in range(num_epochs):\n","        ppl, speed = train_epoch_ch8(net, train_iter, loss, updater,\n","                                     use_random_iter)\n","        if (epoch + 1) % 10 == 0:\n","            print(predict('time traveller'))\n","            animator.add(epoch + 1, [ppl])\n","    device = dl.try_gpu()._device_name\n","    print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')\n","    print(predict('time traveller'))\n","    print(predict('traveller'))"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":55,"tab":["tensorflow"]},"outputs":[],"source":["num_epochs, lr = 500, 1\n","train_ch8(net, train_iter, vocab, lr, num_epochs, strategy)"]},{"cell_type":"code","execution_count":null,"metadata":{"origin_pos":58,"tab":["tensorflow"]},"outputs":[],"source":["with strategy.scope():\n","    net = RNNModelScratch(len(vocab), num_hiddens, init_rnn_state, rnn,\n","                          get_params)\n","train_ch8(net, train_iter, vocab_random_iter, lr, num_epochs, strategy,\n","          use_random_iter=True)"]},{"cell_type":"markdown","metadata":{"origin_pos":59},"source":["## Exercises\n","\n","1. Show that one-hot encoding is equivalent to picking a different embedding for each object.\n","1. Adjust the hyperparameters (e.g., number of epochs, number of hidden units, number of time steps in a minibatch, and learning rate) to improve the perplexity.\n","    * How low can you go?\n","    * Replace one-hot encoding with learnable embeddings. Does this lead to better performance?\n","    * How well will it work on other books by H. G. Wells, e.g., [*The War of the Worlds*](http://www.gutenberg.org/ebooks/36)?\n","1. Modify the prediction function such as to use sampling rather than picking the most likely next character.\n","    * What happens?\n","    * Bias the model towards more likely outputs, e.g., by sampling from $q(x_t \\mid x_{t-1}, \\ldots, x_1) \\propto P(x_t \\mid x_{t-1}, \\ldots, x_1)^\\alpha$ for $\\alpha > 1$.\n","1. Run the code in this section without clipping the gradient. What happens?\n","1. Change sequential partitioning so that it does not separate hidden states from the computational graph. Does the running time change? How about the perplexity?\n","1. Replace the activation function used in this section with ReLU and repeat the experiments in this section. Do we still need gradient clipping? Why?\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"deepnote_notebook_id":"1cf04da2-a4cc-428a-8f67-455a2f25736d"},"nbformat":4,"nbformat_minor":4}